---
title: "Participation in Financial Rulemaking"
#subtitle: "Appendix and Replication Code" 
output:
    bookdown::html_document2:
      highlight: zenburn
      toc: true
      toc_float: true
      code_folding: hide
      number_sections: false
# output:
#   xaringan::moon_reader:
#     lib_dir: libs
#     mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"
#     css: xaringan-themer.css
#     nature:
#       highlightStyle: github
#       highlightLines: true
#       countIncrementalSlides: false
---



```{r options, include=FALSE}
# rmarkdown::render("docs/participation.Rmd")

## Sets defaults for R chunks
knitr::opts_chunk$set(#echo = FALSE, # echo = TRUE means that code will show
                      cache = FALSE,
                      #cache = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.show="hold",
                      fig.pos= "htbp",
                      fig.path = "../figs/",
                      fig.align='center',
                      fig.cap = '   ',
                      fig.retina = 6,
                      fig.height = 3,
                      fig.width = 7,
                      out.width = "100%",
                      out.extra = "")

library("xaringan")
library("xaringanthemer")
library("here")


style_mono_light(base_color = "#3b444b",
          link_color = "#B7E4CF",
          #background_color = "#FAF0E6", # linen
          header_font_google = google_font("PT Sans"), 
          text_font_google = google_font("Old Standard"), 
          text_font_size = "18px",
          padding = "10px",
          code_font_google = google_font("Inconsolata"), 
          code_inline_background_color    = "#F5F5F5", 
          table_row_even_background_color = "#ddede5",
          extra_css = 
            list(".remark-slide-number" = list("display" = "none")))
```

```{r setup}
options(scipen=999)


source(here::here("code", "setup.R"))


library(scales)
library(magrittr)
library(broom)
library(dotwhisker)
library(here)
library(knitr)
library(kableExtra)
library(lmerTest)
library(fixest)
library(modelsummary)
library(tidyverse)

# defaults for plots
library(ggplot2); theme_set(theme_minimal());
options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_color_discrete <- function(...){
  scale_color_viridis_d(..., direction = -1, 
                        begin = 0, end = .6, option = "plasma")}
scale_fill_discrete <- function(...){
  scale_fill_viridis_d(..., direction = -1, 
                       begin = 0, end = .6, option = "plasma")}

scale_color_continuous <- function(...){
  scale_color_viridis_c(..., direction = -1, 
                        option = "plasma")}
scale_fill_continuous <- function(...){
  scale_fill_viridis_c(..., direction = -1, 
                       option = "plasma")}

# Table formatting
library(kableExtra)
kablebox <- . %>% 
  slice_head(n = 100) %>%
  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "400px")
```

# Data
```{r}
# actions 
actions <- read_csv(here("data" , "actions.csv"))

actions %>% distinct(docket_id) %>% nrow

# clean match data from finreg_commenter_covariates
load(here("data", "match_data_clean.Rdata"))

d <- match_data_clean  %>% distinct()

d %<>% mutate(NonProfitTax_name= nonprofits_resources_name,
              # compustat_resources_name,
              OpenSecretsOrgs_name = opensecrets_resources_jwVersion_name)

# data from finreg org level covariates
orgs <- here("data", #"match_data", # old in match_data
              "finreg_org_level_covariates_df_20210908.csv") %>% 
read_csv()

```
# Number of comments by organization type 

```{r org_count_type, fig.width=5, fig.height=2.3}
d %<>% mutate(org_comment = ifelse(is_likely_org == 1, "Likely Org", "Likely Not Org"),
              Agency = str_to_upper(comment_agency))

d %<>% mutate(Org_type = org_type %>% 
                str_replace("CIK", "Filed with SEC (CIK Data)") %>% 
                str_replace("compustat", "Wharton Compustat Company Database") %>% 
                str_replace("FDIC", "FDIC-Insured Bank") %>% 
                str_replace("Nonprofit", "Nonprofit (IRS Data)") %>% 
                str_replace("FFIEC", "FFIEC Bank/Bank-like Entity") %>% 
                str_replace("OpenSecrets", "PAC Donor (CRP Data)"))

# comments 
d %>% group_by(Agency, Org_type) %>% 
  count() %>% 
  ggplot() +
  aes(x = Agency, y = n, fill = Org_type) + 
  geom_col() + 
  labs(fill = "Type of Organanization",
       y = "Number of Comments")


# unique orgs
d %>% distinct(Agency, Org_type, org_name) %>% 
  group_by(Agency, Org_type) %>% 
  count() %>%
  ggplot() +
  aes(x = Agency, y = n, fill = Org_type) + 
  geom_col() + 
  labs(fill = "Type of Organanization",
       y = "Number of Organizations")

# with the data Jacob collapsed
orgs %>% 
  mutate(Agency = str_spl(comment_agency, "\\|"),
         org_type = best_match_type %>% str_remove("_.*")) %>% 
  unnest(Agency) %>%
  group_by(Agency, org_type) %>% 
  count() %>% 
  ggplot() +
  aes(x = Agency, y = n, fill = org_type) + 
  geom_col() + 
  #facet_wrap("org_comment", scales = "free") +
  labs(fill = "Type of Organanization",
       y = "Number of Organizations")

org_count_agency <- d %>% filter(is_likely_org ==1) %>% 
  count(org_name, org_type, org_resources, Agency)

org_count <- d %>% filter(is_likely_org ==1) %>% 
  count(org_name, org_type, org_resources)
```

### Payday loan rule
```{r org_count_type-payday, fig.width=7, fig.height=2.5}
d %<>% mutate(payday = ifelse(str_detect(comment_url, "CFPB-2016-0025|CFPB-2019-0006"), "Payday Loan Rules", "Other Rules"))

d %>% group_by(Agency, Org_type, payday) %>% 
  count() %>% 
  ggplot() +
  aes(x = Agency, y = n, fill = Org_type) + 
  geom_col() + 
  labs(fill = "Type of Organanization",
       y = "Number of Comments") + 
  facet_wrap("payday")

d %>% 
  filter(payday == "Payday Loan Rules") %>% 
  count(comment_org_name, sort = T) 

orgs %>% 
  filter(str_dct(best_match_name, "financial")) %>% 
  arrange(-num_comments)
```

---



# Commenters matched to other datasets of known organizations

```{r match_data}
d %<>% filter(is_likely_org == 1)

# 2k
 paste("Credit Union matches: ", sum(!is.na(d$CreditUnions_name)) )

d %>% filter(!is.na(CreditUnions_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()

# 5k
 paste("CIK matches: ", sum(!is.na(d$CIK_name)) )

d %>% filter(!is.na(CIK_name)) %>% select(ends_with("name"), comment_url)  %>% kablebox()

# 1k
 paste("FDIC_Institutions matches: ", sum(!is.na(d$FDIC_Institutions_name)) )

d %>% filter(!is.na(FDIC_Institutions_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()

# 0
 paste("FFIECInstitutions matches: ", sum(!is.na(d$FFIECInstitutions_name)) )

#d %>% filter(!is.na(FFIECInstitutions_name)) %>% select(ends_with("name")) %>% kablebox()

# 18k
 paste("NonProfitTax matches: ", sum(!is.na(d$NonProfitTax_name)) )

d %>% filter(!is.na(NonProfitTax_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()

# 4k
 paste("OpenSecrets matches: ", sum(!is.na(d$OpenSecretsOrgs_name)) )

d %>% filter(!is.na(OpenSecretsOrgs_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()


# 0
 paste("SEC matches: ", sum(!is.na(d$SEC_Institutions_name)))

# d %>% filter(!is.na(SEC_Institutions_name)) %>% select(ends_with("name"))  %>% kablebox()
 

```


```{r, eval=FALSE}
#Example urls

d %>% group_by(comment_agency) %>% 
  slice(1) %>% 
  select(comment_url)
```

---

#  Number of comments by organization resources

---

## By Assets

(FDIC data)

### Among firms that commented on a Dodd Frank rule

```{r fdic_count, fig.width=4, out.width= "50%"}
max <- org_count %>%   filter(org_type == "FDIC") %>% pull(n) %>% max()

org_count %>% 
  filter(org_type == "FDIC") %>% 
  ggplot() +
  aes(x = org_resources,
      y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() +
  labs(x = "Assets",
       y = "Number of Comments") +
  lims(y =c(0, max))+ 
  scale_x_log10(breaks = breaks_log())

# by agency
max <- org_count_agency %>%   filter(org_type == "FDIC") %>% pull(n) %>% max()

org_count_agency %>% 
  filter(org_type == "FDIC") %>% 
  ggplot() +
  aes(x = org_resources,
      y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() +
  labs(x = "Assets",
       y = "Number of Comments") +
  lims(y =c(0, max)) + 
    facet_wrap("Agency") + 
  scale_x_log10(breaks = breaks_log())
```

---

### Comparing firms that did and did not comment on Dodd-Frank rules


```{r FDIC-density, fig.height=1.5, fig.width=4, out.width="50%"}
load(here("data", "FDIC_resources.Rdata"))

# old match data 
FDIC_resources %>% 
  #filter(assets > 100) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = ASSET/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "FDIC-Insured Banks",
       fill = "", y = "", x = "Assets ($1,000s)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

# new data 
FDIC_resources %<>% 
  mutate(Commented = ifelse(
    org_name %in% orgs$best_match_name,
    "Commented", 
    "Did not comment")) 

FDIC_resources%>% 
  ggplot() + 
  aes(x = ASSET/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "FDIC-Insured Banks",
       fill = "", y = "", x = "Assets ($1,000s)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())
```


```{r fdic-map}
states <- map_data("state")

FDIC_state_n <- FDIC_resources %>% 
  mutate(region = STNAME %>% str_to_lower()) %>% 
  group_by(region) %>% 
  summarise(n = n() )

states %>% 
  left_join(FDIC_state_n) %>% 
  ggplot() +
  aes(long, lat, group = group, fill = n) +
  geom_polygon(color = "white") + 
  labs(fill = "FDIC-Insured Banks") +
  theme_void()


FDIC_state_n <- FDIC_resources %>% 
  mutate(region = STNAME %>% str_to_lower()) %>% 
  group_by(region) %>% 
  summarise(n = sum(Commented == "Commented"))

states %>% 
  left_join(FDIC_state_n) %>% 
  ggplot() +
  aes(long, lat, group = group, fill = n) +
  geom_polygon(color = "white") + 
  labs(fill = "FDIC-Insured Banks \nCommenting On\n Dodd-Frank Rules") +
  theme_void()


```


---

### Among nonprofits that commented on Dodd-Frank rules

```{r nonprofit-count, fig.height=1.5, fig.width=4, out.width="50%"}
load(here("data", "nonprofit_resources.Rdata"))

nonprofit_resources %>% 
  #filter(commented, assets > 100) %>% 
  ggplot() + 
  aes(x = assets, y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() + 
    labs(x = "Assets",
       y = "Number of Comments") +
  scale_x_log10()


nonprofit_resources %>% 
  filter(commented, assets > 100) %>% 
  ggplot() + 
  aes(x = revenue, y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() + 
    labs(x = "Revenue",
       y = "Number of Comments") +
  scale_x_log10() 
```

---

### Comparing nonprofits that did and did not comment on Dodd-Frank rules

```{r nonprofit-density-old, fig.height= 1.5, fig.width=4, out.width="50%"}
load(here("data", "nonprofit_resources.Rdata"))

nonprofit_resources %>% 
  filter(assets > 10) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = assets, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "", x = "Assets")

nonprofit_resources %>% 
  #filter(assets > 100) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = revenue, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "", y = "", x = "Revenue")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

# CHARTS 
# - who comments, who does not 
# - 


# TODO 
# - biggest nonprofits by assets and revenue (e.eg. chamber )
# - compartaitve ratios at key levels
# - percent nonprofits 
# - side by side histograms
# missing fed in matching, but could compare FDIC regulator to agency commented on
# - scholzman and verba, gilens and bartels 
```

Same plots with new data (finreg_org_level_covariates_df_20210908.csv)

```{r nonprofit-density, fig.height=1.5, fig.width=4, out.width="50%", cache=FALSE}
load(here("data", "nonprofit_resources.Rdata"))

nonprofit_resources %<>% 
  mutate(Commented = ifelse(
    org_name %in% orgs$best_match_name,
    "Commented", 
    "Did not comment")) 

nonprofit_resources %>% 
  group_by(Commented) %>% 
  summarise(mean_assets = mean(assets, na.rm = T))

nonprofit_resources %>% 
  filter(assets > 10) %>% 
  ggplot() + 
  aes(x = assets/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Nonprofits",
       fill = "", y = "", x = "Assets ($1,000s)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

nonprofit_resources %>% 
  filter(assets > 10) %>% 
  ggplot() + 
  aes(x = assets/1000, fill = Commented) + 
  geom_histogram(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Nonprofits",fill = "", x = "Assets ($1,000s)") + 
  facet_grid(Commented ~ ., scales = "free_y")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())


nonprofit_resources %>% 
  filter(revenue > 10) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = revenue/1000, fill = Commented) + 
  geom_histogram(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Nonprofits",fill = "", x = "Revenue ($1,000s)")+ 
  facet_grid(Commented ~ ., scales = "free_y")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())
```


---

## By Lobbying Spending

(OpenSecrets data)

```{r opensecrets-density, fig.height=1.5, fig.width=4, out.width="50%"}
opensecrets <- read_csv(here("data" , "merged_resources", 
"opensecrets_resources_JwVersion.csv") )


opensecrets %<>% 
  mutate(org_name = orgName %>% str_to_lower(),
         Commented = ifelse(
    org_name %in% orgs$best_match_name,
    "Commented", 
    "Did not comment"))

opensecrets %>% 
  group_by(Commented) %>% 
  summarise(mean_MeanContribAmount = mean(MeanContribAmount, na.rm = T),
            mean_MeanContribAmountPerYearContributed = mean(MeanContribAmountPerYearContributed, na.rm = T),
            MeanTotalContribAmount = mean(TotalContribAmount, na.rm = T))

opensecrets %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = MeanContribAmount/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "",  
       y = "",
       title = "Campaign Spending",
       x = "Mean Contribution
       ($1,000s, 2010-2017)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

opensecrets %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = MeanContribAmountPerYearContributed/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Campaign Spending",
       fill = "", 
       y = "",
       x = "Mean Contribution\n($1,000s, 2010-2017)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

opensecrets %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = TotalContribAmount/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "", 
       title = "Campaign Spending",
       y = "",
       x = "Total Contributions\n($1,000s, 2010-2017)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())
```


---

## By Market Cap

(Computstat data)

```{r compustat-density, fig.height=1.5, fig.width=4, out.width="50%"}
compustat <- read_csv(here("data/merged_resources/compustat_resources.csv"))


compustat %<>% 
  mutate(org_name = conm %>% str_to_lower(),
         Commented = ifelse(
    org_name %in% orgs$best_match_name,
    "Commented", 
    "Did not comment")) %>% 
  mutate(marketcap2 = ifelse(str_detect(marketcap, "k"),
                             marketcap %>% str_remove("k") %>% as.numeric() * 1000,
                             marketcap),
         marketcap2 = ifelse(str_detect(marketcap, "M"),
                             marketcap %>% str_remove("M") %>% as.numeric() * 1000000,
                             marketcap2),
         marketcap2 = ifelse(str_detect(marketcap, "B"),
                             marketcap %>% str_remove("B") %>% as.numeric() * 1000000000,
                             marketcap2),
         marketcap2 = ifelse(str_detect(marketcap, "T"),
                             marketcap %>% str_remove("T") %>% as.numeric() * 1000000000000,
                             marketcap2),
         marketcap2 = marketcap2 %>% str_remove(",") %>%  as.numeric(marketcap2))

#compustat %>% filter(is.na(marketcap2), !is.na(marketcap)) %>% pull(marketcap)

compustat %>% 
  group_by(Commented) %>%
  #mutate(mean = mean(marketcap2/1000, na.rm = T)) %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = marketcap2/1000, fill = Commented) + 
  labs(fill = "", 
       y = "",
       title = "Banks and Bank-like Entities",
       x = "Market Capitalization\n($1,000s)") + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  #geom_vline(aes(xintercept = mean)) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())
```


# Combine asset data with commenting behavior

```{r}
d %<>% left_join(opensecrets) %>% 
  left_join(nonprofit_resources) %>% 
  left_join(compustat %>% select(-index)) 

d %<>% mutate(org_resources = 
                coalesce(org_resources,
                         MeanContribAmount,
                         assets,
                         marketcap2))

#d$org_resources
```

# Comments per rule

```{r}
# comments 
# of the matched sample 
# 90% of credit union comments, half of nonprofits, and 75% campaig
d %>% 
    add_count(org_type, name = "dataset total") %>% 
  filter(comment_agency == "cfpb") %>% 
  mutate(docket_id = comment_url %>% str_extract("CFPB-[0-9]*-[0-9]*")) %>% 
  count(org_type, docket_id, `dataset total`, sort = T) %>%
  kablebox()

# orgs (15% of nonprifits on payday loan rule, 30% of opensecrets)
d %>% 
  filter(comment_agency == "cfpb") %>% 
  mutate(docket_id = comment_url %>% str_extract("CFPB-[0-9]*-[0-9]*")) %>% 
  distinct(org_name, docket_id, org_type) %>% 
  add_count(org_type, name = "dataset total") %>% 
  count(org_type, docket_id, `dataset total`, sort = T) %>%
  kablebox()

# 
d %>% 
  filter(comment_agency == "cfpb") %>% 
  mutate(docket_id = comment_url %>% str_extract("CFPB-[0-9]*-[0-9]*")) %>% 
  count(org_name, org_type, docket_id, sort = T) %>% kablebox()
```


# Sophistication 

```{r assets-bluebook}
load(here("data", "bluebook.Rdata"))
load(here("data", "tech_iterations.RData"))

technical_terms %<>% mutate(comment_url = doc_id)

# names(bluebook)
# bluebook %>% count(comment_url, sort = T)

d %>% filter(Agency != "CFPB") %>% select(comment_url) %>% kablebox()

  
fix_url <- . %>%
  mutate(
  comment_url = gsub(pattern = "https:/www.sec.gov", 
                     replacement = "https://www.sec.gov", 
                     x = comment_url),
  comment_url = gsub(pattern = "S7-", 
                   replacement = "s7-", 
                   x = comment_url),
  # Federal Reserve Fix
  comment_url = gsub(pattern = "https:\\/\\/www\\.federalreserve\\.gov\\/SECRS\\/\\d{4}\\/\\w+/\\d{1,}/", 
                   replacement = "",
                   x = comment_url)
  )


d %<>% fix_url()

bluebook %<>% fix_url()

technical_terms %<>% fix_url()

bluebook%>%  select(comment_url) %>% kablebox()

d %>% filter(comment_url %in% technical_terms$comment_url)

# 6 k in both sets 
d %>% filter(comment_url %in% bluebook$comment_url)

bluebook %>% filter(comment_url %in% d$comment_url)

# bluebook %>% count(is.na(Total_Legal_Citations))

# 400 from FDIC data, 400 from other data
b <- bluebook %>% 
  left_join(d %>% 
  select(comment_url, org_name, comment_agency, org_resources, MeanContribAmount,
                         assets,
                         marketcap2) %>% 
    distinct() 
) %>% 
  drop_na(Total_Legal_Citations)
# d %>% count(comment_url, sort = T) 

b %>% select(org_name, Total_Legal_Citations, org_resources, comment_url)  %>%  kablebox()

# FDIC ASSTS 
b %>% 
  ggplot() +
  aes(x = org_resources, # %>% log(), 
      y = Total_Legal_Citations) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

# nonprofits
b %>% 
  ggplot() +
  aes(x = assets, 
      y = Total_Legal_Citations) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

# contribution
b %>% 
  ggplot() +
  aes(x = MeanContribAmount, 
      y = Total_Legal_Citations) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

b %>% 
  ggplot() +
  aes(x = marketcap2, 
      y = Total_Legal_Citations) + 
  geom_jitter() + 
  geom_smooth(method = "lm"
  ) 

b %>% filter(Total_Legal_Citations>10) %>% select(Total_Legal_Citations, comment_url) %>% arrange(-Total_Legal_Citations) %>% kablebox()
```

```{r assets-tech, fig.height=2, fig.width=3}
t <- technical_terms %>% 
  left_join(d %>% 
              select(comment_url, org_name,
                     comment_agency, org_resources, MeanContribAmount,
                         assets,
                         marketcap2,
                     `FDIC_Institutions-orgMatch:ASSET`) %>% 
    distinct() 
) %>% 
  drop_na(dictionary_terms)


# FDIC ASSTS 
t %>% 
  filter(!is.na(`FDIC_Institutions-orgMatch:ASSET`)) %>% 
  ggplot() +
  aes(x = `FDIC_Institutions-orgMatch:ASSET`, # %>% log(), 
      y = dictionary_terms ) + 
  geom_point(alpha = .5) + 
  geom_smooth(method = "lm"
  ) + 
  scale_y_log10() +
  scale_x_log10() + 
  labs(x = "Assets",
       y = "Legal and Technial Terms", 
       title = "FDIC-Insured Banks")

# nonprofits
t %>% 
  filter(!is.na(assets)) %>% 
  ggplot() +
  aes(x = assets, 
      y = dictionary_terms) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

# contribution
t %>% 
  filter(!is.na(MeanContribAmount)) %>%
  ggplot() +
  aes(x = MeanContribAmount, 
      y = dictionary_terms) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

t %>% 
  filter(!is.na(marketcap2)) %>% 
  ggplot() +
  aes(x = marketcap2, 
      y = dictionary_terms) + 
  geom_jitter() + 
  geom_smooth(method = "lm"
  ) 
```

This is fishy
```{r}
# fishy
d %>% 
  ggplot() +
  aes(x = marketcap2) + 
  geom_histogram()
```


# Steve's code 
```{r, eval=FALSE}
# Steve's code
# https://www.dropbox.com/s/dt7n0a771jngpte/Efficacy_Plots.R?dl=0
######################################

'%!in%' <- function(x,y)!('%in%'(x,y))

library(scales) # to access break formatting functions
library(DBI)
library(RSQLite)
library(quanteda)
library(tidyverse)
library(magrittr)
library(tidylog)
library(stargazer)
library(corrplot)



all_repeated_language <- read.csv(here("data/repeated_language.csv"), stringsAsFactors = F, 
                                  strip.white = T, sep = ",")

all_repeated_language %<>% as_tibble()

commenter_covariates <- read.csv(file = here("data/finreg_commenter_covariates_df_20210824.csv"), 
                                 strip.white = T, stringsAsFactors = F)

# federal reserve weird in sqlite and that translates to this frame
commenter_covariates %<>%
  mutate(
  comment_url = gsub(pattern = "https:/www.sec.gov", 
                     replacement = "https://www.sec.gov", 
                     x = comment_url),
  comment_url = gsub(pattern = "S7-", 
                   replacement = "s7-", 
                   x = comment_url),
  # Federal Reserve Fix
  comment_url = gsub(pattern = "https:\\/\\/www\\.federalreserve\\.gov\\/SECRS\\/\\d{4}\\/\\w+/\\d{1,}/", 
                   replacement = "",
                   x = comment_url)
  )
  
load(here("data", "tech_iterations.RData"))
load(here("data", "bluebook.RData"))



# Efficacy Measure Summary Stats and Plots ####
technical_terms %<>%
  mutate(  comment_url = doc_id) %>%
  left_join(Tables_and_Figures, by = "comment_url") %>%
  left_join(bluebook, by = "comment_url") %>%
  mutate(
  # SEC fix
  comment_url = gsub(pattern = "https:/www.sec.gov", 
                     replacement = "https://www.sec.gov", 
                     x = comment_url),
  comment_url = gsub(pattern = "S7-", 
                   replacement = "s7-", 
                   x = comment_url),
  # Federal Reserve Fix
  comment_url = gsub(pattern = "https:\\/\\/www\\.federalreserve\\.gov\\/SECRS\\/\\d{4}\\/\\w+/\\d{1,}/", 
                   replacement = "",
                   x = comment_url)
  )
  
technical_terms %>%
  filter(grepl(pattern = "sec.gov",x = comment_url))

missing_from_all_repeated <- all_repeated_language %>%
  anti_join(technical_terms, by = "comment_url")

missing_from_technical_terms <- technical_terms %>%
  anti_join(all_repeated_language, by = "comment_url")

missing_from_technical_terms
  
all_repeated_language %>%
  left_join(technical_terms, by = "comment_url")




## Rename Efficacy Measure variables for plots ####
all_repeated_language %<>%
  
  # Plag sequences
  rename("Num. Seq. Over 5" = "difference_plagiarised_number_of_other_sequences_over_5") %>%
  rename("Num. Seq. Over 10" = "difference_plagiarised_number_of_other_sequences_over_10") %>%
  rename("Num. Seq. Over 25" = "difference_plagiarised_number_of_other_sequences_over_25") %>%
  rename("Num. Seq. Over 50" = "difference_plagiarised_number_of_other_sequences_over_50") %>%
  rename("Num. Seq. Over 100" = "difference_plagiarised_number_of_other_sequences_over_100") %>%
  
  # Plag length
  rename("Seq. Leng. Over 5" = "difference_plagiarised_cumulative_sequence_length_over_5") %>%
  rename("Seq. Leng. Over 10" = "difference_plagiarised_cumulative_sequence_length_over_10") %>%
  rename("Seq. Leng. Over 25" = "difference_plagiarised_cumulative_sequence_length_over_25") %>%
  rename("Seq. Leng. Over 50" = "difference_plagiarised_cumulative_sequence_length_over_50") %>%
  rename("Seq. Leng. Over 100" = "difference_plagiarised_cumulative_sequence_length_over_100") %>%
  
  # Fuzzy sequences
  rename("Fuzzy Num. Seq. Over 10" = "differece_fuzzy_number_of_other_sequences_over_10") %>%
  rename("Fuzzy Num. Seq. Over 25" = "differece_fuzzy_number_of_other_sequences_over_25") %>%
  rename("Fuzzy Num. Seq. Over 50" = "differece_fuzzy_number_of_other_sequences_over_50") %>%
  rename("Fuzzy Num. Seq. Over 100" = "differece_fuzzy_number_of_other_sequences_over_100") %>%
  
  # Fuzzy length
  rename("Fuzzy Seq. Leng. Over 10" = "differece_fuzzy_cumulative_sequence_length_over_10") %>%
  rename("Fuzzy Seq. Leng. Over 25" = "differece_fuzzy_cumulative_sequence_length_over_25") %>%
  rename("Fuzzy Seq. Leng. Over 50" = "differece_fuzzy_cumulative_sequence_length_over_50") %>%
  rename("Fuzzy Seq. Leng. Over 100" = "differece_fuzzy_cumulative_sequence_length_over_100") %>%
  
  # Other terms
  rename("Unique Tokens" = "Types") %>%
  rename("Total Tokens" = "Tokens") %>%
  rename("Total Sentences" = "Sentences") %>%
  rename("Words" = "words") 

order_of_terms_for_plots <- c(
  "Unique Tokens", "Total Tokens", "Total Sentences", "Words",
  "Num. Seq. Over 5", "Num. Seq. Over 10",
  "Num. Seq. Over 25", "Num. Seq. Over 50",
  "Num. Seq. Over 100",
  "Seq. Leng. Over 5", "Seq. Leng. Over 10",
  "Seq. Leng. Over 25", "Seq. Leng. Over 50",
  "Seq. Leng. Over 100",
  "Fuzzy Num. Seq. Over 10","Fuzzy Num. Seq. Over 25",
  "Fuzzy Num. Seq. Over 50", "Fuzzy Num. Seq. Over 100",
  "Fuzzy Seq. Leng. Over 10", "Fuzzy Seq. Leng. Over 25",
  "Fuzzy Seq. Leng. Over 50", "Fuzzy Seq. Leng. Over 100")

data_for_plots <- all_repeated_language %>%
  gather(terms, val, -comment_url) %>%
  mutate(val = as.numeric(val)) %>%
  group_by(terms) %>%
  filter(terms %in% order_of_terms_for_plots) %>%
  mutate(terms = factor(terms, levels=c(
    "Unique Tokens",
    "Total Tokens",
    "Total Sentences",
    "Words",
    "Num. Seq. Over 5",
    "Num. Seq. Over 10",
    "Num. Seq. Over 25",
    "Num. Seq. Over 50",
    "Num. Seq. Over 100",
    "Seq. Leng. Over 5",
    "Seq. Leng. Over 10",
    "Seq. Leng. Over 25",
    "Seq. Leng. Over 50",
    "Seq. Leng. Over 100",
    "Fuzzy Num. Seq. Over 10",
    "Fuzzy Num. Seq. Over 25",
    "Fuzzy Num. Seq. Over 50",
    "Fuzzy Num. Seq. Over 100",
    "Fuzzy Seq. Leng. Over 10",
    "Fuzzy Seq. Leng. Over 25",
    "Fuzzy Seq. Leng. Over 50", 
    "Fuzzy Seq. Leng. Over 100")))

#### Summary Stats ####
summary_stats <- data_for_plots %>%
  summarise(
    n = n(),
    min = fivenum(val)[1],
    #Q1 = fivenum(val)[2],
    median = fivenum(val)[3],
    #Q3 = fivenum(val)[4],
    Q90 = quantile(val, 0.9, na.rm = T),
    Q95 = quantile(val, 0.95, na.rm = T),
    Q99 = quantile(val, 0.99, na.rm = T),
    max = fivenum(val)[5]
  ) 

summary_stats %>%
  pander::pandoc.table()

#### Export Summary Stats to Latex ####
print(xtable::xtable(summary_stats, digits = 0), include.rownames = F)

# Violin Plots ####

#### Length ####
comment_length <- data_for_plots %>% 
  filter(terms %in% c("Total Sentences","Words")) %>%
  mutate(val = val + 1) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  facet_wrap(~terms, scales = "free") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_minimal() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Comment Length",
       caption = "Sentences measured from capital letter to period; Words measured as characters between spaces") +  
  theme(legend.position="none",
        plot.title = element_text(size = 12, face = "bold",hjust = 0.5)) +  
  xlab("") +
  ylab("Count")

#### Exact Matches: Number ####
plag_seq <- data_for_plots %>%
  filter(terms %in% c(                        "Num. Seq. Over 5",
                                              "Num. Seq. Over 10",
                                              "Num. Seq. Over 25",
                                              "Num. Seq. Over 50",
                                              "Num. Seq. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Exact Matches: Number of Sequences over Length n in Comment and Final Rule",
       caption = "These are counts; a 1 where n = 5 represents 5 words in the comment and final rule") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Count of Exact Sequences")

#### Exact Matches: Length ####
plag_len <- data_for_plots %>%  
  filter(terms %in% c("Seq. Leng. Over 5",
                      "Seq. Leng. Over 10",
                      "Seq. Leng. Over 25",
                      "Seq. Leng. Over 50",
                      "Seq. Leng. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Exact Matches: Total Length of Matched Sequences",
       caption = "These are the cumulative lengths of the sequences above per document") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Total Amount of Repeated Text")

#### Fuzzy Matches: Number ####
fuzzy_seq <- data_for_plots %>%  
  filter(terms %in% c("Fuzzy Num. Seq. Over 10",
                      "Fuzzy Num. Seq. Over 25",
                      "Fuzzy Num. Seq. Over 50",
                      "Fuzzy Num. Seq. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Fuzzy Matches: Number of Sequences over Length n in Comment and Final Rule",
       caption = "These are counts; a 1 where n = 5 represents 5 words in the comment and final rule") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Count of Fuzzy Mathed Sequences")

#### Fuzzy Matches: Length ####
fuzzy_len <- data_for_plots %>%  
  filter(terms %in% c("Fuzzy Seq. Leng. Over 10",
                      "Fuzzy Seq. Leng. Over 25",
                      "Fuzzy Seq. Leng. Over 50", 
                      "Fuzzy Seq. Leng. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Fuzzy Matches: Total Length of Matched Sequences",
       caption = "These are the cumulative lengths of the sequences above per document") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Total Amount of Repeated Text")

## Save Distribution Plots ####
plag_seq
plag_len
fuzzy_seq
fuzzy_len

# ggsave(plot = plag_seq, "violin_plag_seq.pdf", width = 9, height = 6.5)
# ggsave(plot = plag_seq, "violin_plag_len.pdf", width = 9, height = 6.5)
# ggsave(plot = fuzzy_len, "violin_fuzzy_seq.pdf", width = 9, height = 6.5)
# ggsave(plot = fuzzy_seq, "violin_fuzzy_seq.pdf", width = 9, height = 6.5)

# Correlation Plots ####
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  return(p.mat)
}

assemble_corr_plots <- function(tbl){
  # tbl = data_for_correlation_plot 
  
  #names(tbl) <- base::tolower(substring(names(tbl), 1,5))
  
  corrs <- tbl %>%
    cor()
  
  p.mat <-  tbl %>%
    cor.mtest()
  
  col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
  
  c_plot <- function() {
    corrplot(corrs, method="color", col=col(200),  
             type="upper", order="hclust", 
             addCoef.col = "black", # Add coefficient of correlation
             tl.col="black", tl.srt=45, #Text label color and rotation
             # Combine with significance
             p.mat = p.mat, sig.level = 0.01, insig = "blank", 
             # hide correlation coefficient on the principal diagonal
             diag=FALSE)
  }
  list(c_plot = c_plot)
}

## Length ####
length_variables <- all_repeated_language %>%
  select(`Unique Tokens`,`Total Tokens`,
         `Total Sentences`, `Words`) 

pdf(file = "Length_Correlations.pdf")
assemble_corr_plots(length_variables)$c_plot()
dev.off()

plagiarism_number_variables <- all_repeated_language %>%
  select(`Num. Seq. Over 5`,`Num. Seq. Over 10`,`Num. Seq. Over 25`,
         `Num. Seq. Over 50`, `Num. Seq. Over 100`) 

pdf(file = "Plagiarism_Number_Correlations.pdf")
assemble_corr_plots(plagiarism_number_variables)$c_plot()
dev.off()

plagiarism_length_variables <- all_repeated_language %>%
  select(`Seq. Leng. Over 5`, `Seq. Leng. Over 10`,
         `Seq. Leng. Over 25`,`Seq. Leng. Over 50`,
         `Seq. Leng. Over 100`) 

#pdf(file = "Plagiarism_Length_Correlations.pdf")
#assemble_corr_plots(plagiarism_length_variables)$c_plot()
dev.off()

fuzzy_number_variables <- all_repeated_language %>%
  select(`Fuzzy Num. Seq. Over 10`,
         `Fuzzy Num. Seq. Over 25`,
         `Fuzzy Num. Seq. Over 50`,
         `Fuzzy Num. Seq. Over 100`) 

pdf(file = "Fuzzy_Number_Correlations.pdf")
#assemble_corr_plots(fuzzy_number_variables)$c_plot()
dev.off()

### Fuzzy Length of Sequences ###
fuzzy_length_variables <- all_repeated_language %>%
  select(`Fuzzy Seq. Leng. Over 10`,
         `Fuzzy Seq. Leng. Over 25`,
         `Fuzzy Seq. Leng. Over 50`,
         `Fuzzy Seq. Leng. Over 100`) 

pdf(file = "Fuzzy_Length_Correlations.pdf")
#assemble_corr_plots(fuzzy_length_variables)$c_plot()
dev.off()

all <- all_repeated_language %>%
  select(`Unique Tokens`,`Total Tokens`,
         `Total Sentences`, `Words`,
    `Num. Seq. Over 5`,`Num. Seq. Over 10`,`Num. Seq. Over 25`,
         `Num. Seq. Over 50`, `Num. Seq. Over 100`,
         `Seq. Leng. Over 5`, `Seq. Leng. Over 10`,
         `Seq. Leng. Over 25`,`Seq. Leng. Over 50`,
         `Seq. Leng. Over 100`,
         `Fuzzy Num. Seq. Over 10`,
         `Fuzzy Num. Seq. Over 25`,
         `Fuzzy Num. Seq. Over 50`,
         `Fuzzy Num. Seq. Over 100`,
         `Fuzzy Seq. Leng. Over 10`,
         `Fuzzy Seq. Leng. Over 25`,
         `Fuzzy Seq. Leng. Over 50`,
         `Fuzzy Seq. Leng. Over 100`) 

pdf(file = "All_Correlations.pdf",width=11, height=8.5, paper = "USr")
#assemble_corr_plots(all)$c_plot()
dev.off()

rm(all, fuzzy_length_variables,fuzzy_number_variables,
   plagiarism_length_variables,plagiarism_number_variables)

# Create two dataframes, one wide and one long ####

wide<- all_repeated_language %>%
  inner_join(commenter_covariates, by = "comment_url")

### Lots of missing comments in commenter_covariates
missing <- all_repeated_language %>%
  anti_join(commenter_covariates, by = "comment_url") 

# missing %>% View()

commenter_covariates$comment_url[201316]

### see one example here:
grep(pattern = "CFPB-2016-0020-0044", x = all_repeated_language$comment_url)
grep(pattern = "CFPB-2016-0020-0044", x = commenter_covariates$comment_url)

#### Asset Scatterplots ####
mdc_and_efficacy <- all_repeated_language %>%
  inner_join(match_data_clean, by = "comment_url") 

Words <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=Words)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Comment Length and Organizational Resources",
       caption = "") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Words")


Plag_Count_Over_10 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Num. Seq. Over 10`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Number of Repeated Sequences over 10 and Organizational Resources",
       caption = "The depentend variable is count") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")

Plag_Count_Over_25 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Num. Seq. Over 25`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Number of Repeated Sequences over 25 and Organizational Resources",
       caption = "The depentend variable is count") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")

Plag_Length_Over_10 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Seq. Leng. Over 10`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Cumulative Length and Organizational Resources",
       caption = "The depentend variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")

Plag_Length_Over_25 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Seq. Leng. Over 25`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Cumulative Length and Organizational Resources",
       caption = "The depentend variable is based on sequences of over 25 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")


# master_location <- "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/attachments.sqlite"
# con = dbConnect(SQLite(), dbname=master_location)
# myQuery <- dbSendQuery(con, "SELECT * FROM attachments")
# All_comment_text <- dbFetch(myQuery, n = -1) %>% as_tibble()

# All_comment_text %<>%  arrange(comment_url)

check_no_words <- mdc_and_efficacy %>%
  select(comment_url, Words, org_resources) %>%
  filter(Words == 0) %>%
  select(comment_url) %>%
  pull()

# All_comment_text %>%
#   filter(comment_url %in% check_no_words)

# rm(All_comment_text)

Words
Plag_Count_Over_10
Plag_Count_Over_25
Plag_Length_Over_10
Plag_Length_Over_25
# ggsave(plot = Words, "Assets_Words.pdf", width = 9, height = 6.5)
# ggsave(plot = Plag_Count_Over_10, "Assets_Plag_Count_10.pdf", width = 9, height = 6.5)
# ggsave(plot = Plag_Count_Over_25, "Assets_Plag_Count_25.pdf",width = 9, height = 6.5)
# ggsave(plot = Plag_Length_Over_10, "Assets_Plag_Length_10.pdf", width = 9, height = 6.5)
# ggsave(plot = Plag_Length_Over_25, "Assets_Plag_Length_25.pdf", width = 9, height = 6.5)


repeated_text_scatter <- wide %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         `Seq. Leng. Over 10`, Words) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  gather(match_type, resources, -comment_url, -best_match_type, 
         -`Repeated Text`,
         -Words) %>%
  ggplot( aes(x=resources, y=`Repeated Text`)) + #fill = best_match_type, color = best_match_type
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~match_type, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Cumulative Repeated Text and Organizational Resources",
       caption = "The depentend variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Total Amount of Repeated Text")

repeated_text_by_agency <- wide %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         comment_agency,
         `Seq. Leng. Over 10`, Words) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  gather(match_type, resources, -comment_url, -best_match_type, 
         -`Repeated Text`,-comment_agency,
         -Words) %>%
  ggplot( aes(x=resources, y=`Repeated Text`)) + #fill = best_match_type, color = best_match_type
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Cumulative Repeated Text and Organizational Resources",
       caption = "The dependent variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Total Amount of Repeated Text (Words)")

# Plots for paper ####

# ggsave(plot = repeated_text_by_agency, "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Figures/Repeated_text_agency_assets_cumulative_over10.pdf", width = 13, height = 6.5)

# ggsave(plot = repeated_text_scatter, "Repeated_text_assets_cumulative_over10.pdf", width = 9, height = 6.5)

rpt_naive <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure", data = wide)
rpt_w_control <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)", data = wide)
rpt_w_control_w_fex <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
                          comment_agency + rin", data = wide)
rpt_w_control_w_full_fex <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
                          comment_agency + rin + best_match_type", data = wide)

# library(miceadds)

#rpt_w_control_se <- lm.cluster(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)", data = wide,cluster = 'rin')
#rpt_w_control_w_fex_se <- lm.cluster(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
#                          comment_agency + rin", data = wide,cluster = 'rin')
#rpt_w_control_w_full_fex_se <- lm.cluster(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
#                          comment_agency + rin + #best_match_type", data = wide,cluster = 'rin')
# 
# stargazer::stargazer(rpt_naive,rpt_w_control,rpt_w_control_w_fex, rpt_w_control_w_full_fex,
#                      title="Text Reuse and Organizational Assets", type="latex", 
#                      digits=3, 
#                      se = list(NULL, 
#                                sqrt(diag(vcov(rpt_w_control_se))), 
#                                sqrt(diag(vcov(rpt_w_control_w_fex_se))), 
#                                sqrt(diag(vcov(rpt_w_control_w_full_fex_se))) 
#                                ),
#                      dep.var.labels=c("Cumulative Repeated Text"),
#                      omit = c("comment_agency","rin","best_match_type"),
#                      covariate.labels=c("Log Org. Resources",
#                                         "Log Comment Length"),
#                      omit.stat=c("LL","ser","f","adj.rsq"), no.space=TRUE,
#                      add.lines = list(
#                        c("Clustering", "No", "Rule","Rule","Rule"),
#                        c("Fixed effects?", "No", "No","Agency, RIN","Agency, RIN, Source")),
#                      #out="/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Tables/efficacy_and_resources.txt"
)

# Complexity plots ####
tables_and_figures <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  ggplot( aes(x=Visualizations, y=`Repeated Text`)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Log Cumulative Repeated Text and Data",
       caption = "The dependent variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Tables and Figures") +
  ylab("Total Amount of Repeated Text (Words)")
  
# Dictionary Term plots ####
dict_terms <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  ggplot( aes(x=dictionary_terms, y=`Repeated Text`)) +
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Cumulative Repeated Text and Legal and Banking Terms",
       caption = "The dependent variable is based on sequences of over 10 words \n 
       Dictionary terms are the sum of banking and legal terms") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Legal and Banking Terms") +
  ylab("Total Amount of Repeated Text (Words)")

bluebook <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  ggplot( aes(x=Total_Legal_Citations, y=`Repeated Text`)) +
  #Total_Legal_Citations = US_Code + Supreme_Court_Cases + 
  #Appeals_and_District_Court_Cases + Code_of_Federal_Regulations +
  #Federal_Register_Total
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Cumulative Repeated Text and Bluebook Citations",
       caption = "The dependent variable is based on sequences of over 10 words \n
       Bluebook terms are include US Code, Federal Court Cases, the CFR") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Bluebook Citations") +
  ylab("Total Amount of Repeated Text (Words)")

# ggsave(plot = dict_terms, "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Figures/Repeated_text_agency_dictionary_cumulative_over10.pdf", width = 13, height = 6.5)
# ggsave(plot = bluebook, "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Figures/Repeated_text_agency_bluebook_cumulative_over10.pdf", width = 13, height = 6.5)

big_dict_resources <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         comment_agency,
         `Seq. Leng. Over 10`,dictionary_terms) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  gather(match_type, resources, -dictionary_terms,-comment_url, -best_match_type, 
         -comment_agency) %>%
  filter(resources>10000) %>%
  ggplot( aes(x=resources, y=dictionary_terms)) +
  #Total_Legal_Citations = US_Code + Supreme_Court_Cases + 
  #Appeals_and_District_Court_Cases + Code_of_Federal_Regulations +
  #Federal_Register_Total
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Dictionary Terms and Resources",
       caption = "") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Comment Sophistication (Dictionary Terms)")

all_dict_resources <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         comment_agency,
         `Seq. Leng. Over 10`,dictionary_terms) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  gather(match_type, resources, -dictionary_terms,-comment_url, -best_match_type, 
         -comment_agency) %>%
  ggplot( aes(x=resources, y=dictionary_terms)) +
  #Total_Legal_Citations = US_Code + Supreme_Court_Cases + 
  #Appeals_and_District_Court_Cases + Code_of_Federal_Regulations +
  #Federal_Register_Total
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Dictionary Terms and Resources",
       caption = "") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Comment Sophistication (Dictionary Terms)")


names(technical_terms)

#'4. @Steve and @Chris, I’d recommend some scatters (colored by data source: “best_match_type”) 
#'and regressions of comment sophistication by log_p1_financial_measure 
#'(and in the organization-level dataset, by num_comments submitted and by 
#'num_venues_commenting_in)


#### DEAL WITH THIS LATER ####
long <- all_repeated_language %>%
  inner_join(commenter_covariates, by = "comment_url") 



```
