---
title: "Participation in Financial Rulemaking"
#subtitle: "Appendix and Replication Code" 
output:
    bookdown::html_document2:
      highlight: zenburn
      toc: true
      toc_float: true
      code_folding: hide
      number_sections: false
# output:
#   xaringan::moon_reader:
#     lib_dir: libs
#     mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"
#     css: xaringan-themer.css
#     nature:
#       highlightStyle: github
#       highlightLines: true
#       countIncrementalSlides: false
---



```{r options, include=FALSE}
# rmarkdown::render("docs/participation.Rmd")

## Sets defaults for R chunks
knitr::opts_chunk$set(#echo = FALSE, # echo = TRUE means that code will show
                      cache = FALSE,
                      #cache = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.show="hold",
                      fig.pos= "htbp",
                      fig.path = "../figs/",
                      fig.align='center',
                      fig.cap = '   ',
                      fig.retina = 1, #6, #FIXME for publication quality images
                      fig.height = 3,
                      fig.width = 7,
                      out.width = "100%",
                      out.extra = "")

library("xaringan")
library("xaringanthemer")
library("here")
library("modelsummary")

# table defaults
modelsummary <- function(...) modelsummary::modelsummary(..., 
                                                         stars = T, 
                                                         add_rows = rows,
                                                         coef_rename = cm,
                                                         coef_omit = "(Intercept)") %>% 
  row_spec(row = 1, bold = T)

# coeficient plot defaults
modelplot <- function(...) modelsummary::modelplot(..., coef_omit = "(Intercept)") + 
  geom_vline(xintercept = 0, linetype = 2, alpha = .5)

style_mono_light(base_color = "#3b444b",
          link_color = "#B7E4CF",
          #background_color = "#FAF0E6", # linen
          header_font_google = google_font("PT Sans"), 
          text_font_google = google_font("Old Standard"), 
          text_font_size = "18px",
          padding = "10px",
          code_font_google = google_font("Inconsolata"), 
          code_inline_background_color    = "#F5F5F5", 
          table_row_even_background_color = "#ddede5",
          extra_css = 
            list(".remark-slide-number" = list("display" = "none")))
```

```{r setup}
options(scipen=999)

source(here::here("code", "setup.R"))

# defaults for plots
library(ggplot2); theme_set(theme_minimal());
options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_color_discrete <- function(...){
  scale_color_viridis_d(..., direction = -1, 
                        begin = 0, end = .6, option = "plasma")}
scale_fill_discrete <- function(...){
  scale_fill_viridis_d(..., direction = -1, 
                       begin = 0, end = .6, option = "plasma")}

scale_color_continuous <- function(...){
  scale_color_viridis_c(..., direction = -1, 
                        option = "plasma")}
scale_fill_continuous <- function(...){
  scale_fill_viridis_c(..., direction = -1, 
                       option = "plasma")}

# Table formatting
library(kableExtra)
kablebox <- . %>% 
  slice_head(n = 300) %>%
  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "400px")
```

# Data
```{r}
# actions 
actions <- read_csv(here("data" , "actions.csv"))

actions %>% distinct(docket_id) %>% nrow

library("readxl")

rins <- read_excel(here::here("data", "dodd-frank-rulemaking-directory_doc_oriented_wdescription.xls")) %>% 
  rename(docket_id = identifier)  %>% 
  select(-"...1")

lookup <- read_csv(here::here("data", "docket_related_rin_lookup.csv")) %>% 
  select(-"...1") %>% 
  mutate(across(everything(), str_squish))

rin_lookup <- full_join(rins, lookup, by = "docket_id", suffix = c(".directory", ".lookup")) %>% 
  mutate(title = str_to_title(title) %>% str_squish()) %>% 
  mutate(across(everything(), str_squish))

```

Some dockets don't have related rins in lookup table 
```{r}
# ~35/2 with different comment counts per docket 
# ~325/2 with different titles per docket 
# ~98/2 with different related RINS / lowest related rin per docket 
# 115/2 with different docket_urls

# missing related RINs
rin_lookup %>% 
  select(# title, 
    docket_id,
    RIN.directory, RIN.lookup,
         related.rins, 
         lowest.sorted.rin.of.related.rins, 
         docket_url.directory, docket_url.lookup,
             comment_count) %>%
  distinct() %>% 
  filter(is.na(related.rins)) %>% 
  kablebox()
```

### Dockets with more than one RIN
```{r}
rin_lookup %>% 
  select(# title, 
    docket_id,
    RIN.directory, RIN.lookup,
         related.rins, 
         lowest.sorted.rin.of.related.rins, 
         docket_url.directory, docket_url.lookup,
             comment_count) %>%
  distinct() %>% 
  filter(!is.na(related.rins)) %>% 
  add_count( docket_id) %>%
  arrange(docket_id) %>% 
  filter(n > 1) %>%   
  distinct() %>% 
  kablebox()

rin_lookup %<>% 
  select(-RIN.directory,
         -docket_url.directory) %>% distinct()

names(rin_lookup) %<>% str_remove(".lookup")
```


## Asset data

Summary table data here: 

```{r docket_table, fig.height=5.5, fig.width=5.4, out.width="80%"}
# clean match data from finreg_commenter_covariates
docket_table <- read_csv(here::here("data", "matches_per_docket.csv"))


docket_table %>% 
  add_count(Agency) %>% 
  mutate(Agency = paste(n, "dockets from", Agency)) %>% 
  select(- Percent, - Docket, -`Any Resources Measure`) %>% 
  group_by(Agency) %>% 
  summarise_all(sum) %>% 
  group_by(Agency) %>% 
  pivot_longer(cols = c("Comments", contains(" ") ))  %>% 
  mutate(Matched = ifelse(name == "Comments", "Total","Matched"),
         name = ifelse(name == "Comments", "Total", name)) %>% 
  ggplot() + 
  aes(fill = name,
      y = Matched, 
      x = value) + 
    geom_col(position = "stack", alpha = .7) + 
  labs(x = "Comments", y = "", fill = "Resource\nData Type",
       title = "Comments Matched to Organizational Resources Data") + 
  facet_wrap("Agency", ncol = 1, scales = "free") + 
  theme(legend.position = "bottom",
        panel.grid.major.y = element_blank(),
        strip.text = element_text(size = 12)) 
  

docket_table %>% 
  knitr::kable() %>% 
  kable_paper(full_width = F) %>%
  column_spec(5, color = "white",
              background = spec_color(docket_table$Comments/ docket_table$`Any Resources Measure`, begin = .1, end = .9, direction = 1, option = 3) ) %>% 
  scroll_box(height = "400px")
```

```{r}
# clean match data from finreg_commenter_covariates
load(here("data", "match_data_clean.Rdata"))

d <- match_data_clean  
```

# Number of comments by organization type 

```{r org_count_type, fig.width=5.5, fig.height=2.3}
d %<>% mutate(#org_comment = ifelse(is_likely_org == 1, "Likely Org", "Likely Not Org"),
              Agency = str_to_upper(comment_agency))

#d %<>% filter(is_likely_org == 1)

#FIXME changing this 
d %<>% mutate(org_type = best_match_type)

d %<>% mutate(Org_type = org_type %>% 
                str_replace("CIK", "Filed with SEC (CIK Data)") %>% 
                str_replace("Compustat", "Wharton Compustat Company Database") %>% 
                str_replace("FDIC", "FDIC-Insured Bank") %>% 
                str_replace("Nonprofit", "Nonprofit (IRS Data)") %>% 
                str_replace("FFIEC", "FFIEC Bank/Bank-like Entity") %>% 
                str_replace("OpenSecrets", "PAC Donor (CRP Data)"))

# comments 
d %>% group_by(Agency, Org_type) %>% 
  count() %>% 
  ggplot() +
  aes(x = Agency, y = n, fill = Org_type) + 
  geom_col() + 
  labs(fill = "Type of Organanization",
       y = "Number of Comments")


# using best_match_type instead of org_type
#FIXME WHY IS THERE A DIFFERENCE
# d %>% 
#   mutate(Agency = str_spl(comment_agency, "\\|"),
#          org_type = Best_match_type %>% str_remove("_.*")) %>% 
#   unnest(Agency) %>%
#   group_by(Agency, org_type) %>% 
#   count() %>% 
#   ggplot() +
#   aes(x = Agency, y = n, fill = org_type) + 
#   geom_col() + 
#   #facet_wrap("org_comment", scales = "free") +
#   labs(fill = "Type of Organanization",
#        y = "Number of Comments")

# unique orgs 
d %>% distinct(Agency, Org_type, org_name) %>% 
  group_by(Agency, Org_type) %>% 
  count() %>%
  ggplot() +
  aes(x = Agency, y = n, fill = Org_type) + 
  geom_col() + 
  labs(fill = "Type of Organanization",
       y = "Number of Organizations")


org_count_agency <- d %>% 
  count(org_name, org_type, org_resources, Agency)

org_count <- d  %>% 
  count(org_name, org_type, org_resources)
```

---

# Comments per docket

```{r rins-table}
d_rin_lookup <- d %>% full_join(rin_lookup) %>% 
  distinct(docket_id,
         #  RIN,
         related.rins, 
         lowest.sorted.rin.of.related.rins, 
         #docket_url,  
         comment_count,
         comment_url) %>% 
  group_by(docket_id) %>%
  mutate(comments_matched = sum(!is.na(comment_url) ) ) %>% 
  mutate(comment_count = as.numeric(comment_count)) %>% 
  select(docket_id, comment_count,comments_matched, everything()) %>% 
  arrange(docket_id) %>% 
  filter(!is.na(docket_id))

d_rin_lookup  %>% 
  select(-comment_url) %>% 
  distinct() %>% 
  #filter(comments_matched > 0)
  kablebox()
```

Some of these appear to be over-matched, but it is also possible that the comment counts are wrong. 

```{r rins-table-comments-counts}
# 6 over matched 
d_rin_lookup %>% 
  filter(comment_count < comments_matched) %>%
  #arrange(comment_url) %>% 
  select(-comment_url) %>% 
  distinct() %>% 
  kablebox()
```

---

### Payday loan rule

The payday loan rule accounts for 90% of credit union comments and 50% of nonprofit comments, and 75% of open secrets matched comments in the current data.
15% of nonprofits and 30% of opensecrets matches commented only on the payday loan rule

```{r org_count_type-payday, fig.width=7.5, fig.height=2.5}
d %<>% mutate(payday = ifelse(str_detect(comment_url, "CFPB-2016-0025|CFPB-2019-0006"), "Payday Loan Rules", "All Other Rules"))

d %>% group_by(Agency, Org_type, payday) %>% 
  count() %>% 
  ggplot() +
  aes(x = Agency, y = n, fill = Org_type) + 
  geom_col() + 
  labs(fill = "Type of Organanization",
       y = "Number of Comments") + 
  facet_wrap("payday")

# number of payday loan comments per org 
d %>% 
  #filter(payday == "Payday Loan Rules") %>% 
  count(comment_org_name, best_match_name, sort = T) %>% 
  kablebox() 

```

# Potential false matches

```{r false-matches}

false_match <- "nknown|private investor|private individual|self employed|american citizen|us citizen|advance america|b d | store 7|debt trap|financial 3|united states congress|lending bear|check city partnership|plan nevada|clarity services|title cash|st louis community credit union|antipoverty coalition greater dallas|faith fair lending|south carolina state senate|florida alliance consumer protection|minneapolis grain exchange|asset management group securities industry financial markets association|american title escrow|ssociation financial guaranty insurers|american financial services association|credit union association dakotas|working group commercial energy firms|center capital markets competitiveness|chelsea title nature coast|kendall county|california nevada credit union leagues|futures industry association fia|national rural electric cooperative association|pennsylvania housing finance agency|members congress|new york portfolio clearing|check city partnership|central title|committee on investment employee benefit assets|futures industry association|national association independent housing professionals"

# possible false positives
d %>% filter(str_detect(comment_org_name, false_match)) %>% nrow() %>% paste(" potential false matches")

# false positive orgs 
d %>% filter(str_detect(comment_org_name, false_match)) %>% 
  count(comment_org_name, best_match_name, best_match_score, best_match_type, sort = T) %>% 
  #kable(format = "markdown")
  kablebox()

# false positive comments 
d %>% filter(str_detect(comment_org_name, false_match)) %>% 
  kablebox()


# drop false matches 
d %<>% filter(!str_detect(comment_org_name, false_match))


d %>% 
  filter(str_dct(best_match_name, "financial")) %>% 
  arrange(-org_comments) %>% 
  select(org_comments, comment_org_name,  best_match_name, best_match_score, best_match_type, comment_url, everything()) %>% 
  kablebox()
```

---



# Commenters matched to other datasets of known organizations

```{r match_data}
# 2k
 paste("Credit Union matches: ", sum(!is.na(d$CreditUnions_name)) )

d %>% filter(!is.na(CreditUnions_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()

# 5k
 paste("CIK matches: ", sum(!is.na(d$CIK_name)) )

d %>% filter(!is.na(CIK_name)) %>% select(ends_with("name"), comment_url)  %>% kablebox()

# 1k
 paste("FDIC_Institutions matches: ", sum(!is.na(d$FDIC_Institutions_name)) )

d %>% filter(!is.na(FDIC_Institutions_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()

# 0
 paste("FFIECInstitutions matches: ", sum(!is.na(d$FFIECInstitutions_name)) )

#d %>% filter(!is.na(FFIECInstitutions_name)) %>% select(ends_with("name")) %>% kablebox()

# 18k
 paste("NonProfitTax matches: ", sum(!is.na(d$nonprofits_resources_name)) )

d %>% filter(!is.na(nonprofits_resources_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()

# 4k
 paste("OpenSecrets matches: ", sum(!is.na(d$opensecrets_resources_name)) )

d %>% filter(!is.na(opensecrets_resources_name)) %>% select(ends_with("name"), comment_url) %>% kablebox()


# 0
 paste("SEC matches: ", sum(!is.na(d$SEC_Institutions_name)))

# d %>% filter(!is.na(SEC_Institutions_name)) %>% select(ends_with("name"))  %>% kablebox()
 

```

Example URLs
```{r, eval=FALSE}
#Example urls

d %>% group_by(comment_agency) %>% 
  slice(1) %>% 
  select(comment_url) 
```

---

#  Number of comments by organization resources

---

## By Assets

(FDIC data)

### Among firms that commented on a Dodd Frank rule

```{r fdic_count, fig.width=4, out.width= "50%"}
max <- org_count %>%   filter(org_type == "FDIC") %>% pull(n) %>% max()

org_count %>% 
  filter(org_type == "FDIC") %>% 
  ggplot() +
  aes(x = org_resources,
      y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() +
  labs(x = "Assets",
       y = "Number of Comments") +
  lims(y =c(0, max))+ 
  scale_x_log10(breaks = breaks_log())

# by agency
max <- org_count_agency %>%   filter(org_type == "FDIC") %>% pull(n) %>% max()

org_count_agency %>% 
  filter(org_type == "FDIC") %>% 
  ggplot() +
  aes(x = org_resources,
      y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() +
  labs(x = "Assets",
       y = "Number of Comments") +
  lims(y =c(0, max)) + 
    facet_wrap("Agency") + 
  scale_x_log10(breaks = breaks_log())
```

---

### Comparing firms that did and did not comment on Dodd-Frank rules


```{r FDIC-density, fig.height=1.5, fig.width=4, out.width="50%"}
load(here("data", "FDIC_resources.Rdata"))

# old match data 
FDIC_resources %>% 
  #filter(assets > 100) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = ASSET/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "FDIC-Insured Banks",
       fill = "", y = "", x = "Assets ($1,000s)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

# new data 
FDIC_resources %<>% 
  mutate(Commented = ifelse(
    org_name %in% d$best_match_name,
    "Commented", 
    "Did not comment")) 

FDIC_resources%>% 
  ggplot() + 
  aes(x = ASSET/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "FDIC-Insured Banks",
       fill = "", y = "", x = "Assets ($1,000s)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())
```


```{r fdic-map}
states <- map_data("state")

FDIC_state_n <- FDIC_resources %>% 
  mutate(region = STNAME %>% str_to_lower()) %>% 
  group_by(region) %>% 
  summarise(n = n() )

states %>% 
  left_join(FDIC_state_n) %>% 
  ggplot() +
  aes(long, lat, group = group, fill = n) +
  geom_polygon(color = "white") + 
  labs(fill = "FDIC-Insured Banks") +
  theme_void()


FDIC_state_n <- FDIC_resources %>% 
  mutate(region = STNAME %>% str_to_lower()) %>% 
  group_by(region) %>% 
  summarise(n = sum(Commented == "Commented"))

states %>% 
  left_join(FDIC_state_n) %>% 
  ggplot() +
  aes(long, lat, group = group, fill = n) +
  geom_polygon(color = "white") + 
  labs(fill = "FDIC-Insured Banks \nCommenting On\n Dodd-Frank Rules") +
  theme_void()


```


---

### Among nonprofits that commented on Dodd-Frank rules

```{r nonprofit-count, fig.height=1.5, fig.width=4, out.width="50%"}
load(here("data", "nonprofit_resources.Rdata"))

nonprofit_resources %>% 
  #filter(commented, assets > 100) %>% 
  ggplot() + 
  aes(x = assets, y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() + 
    labs(x = "Assets",
       y = "Number of Comments") +
  scale_x_log10()


nonprofit_resources %>% 
  filter(commented, assets > 100) %>% 
  ggplot() + 
  aes(x = revenue, y = n) + 
  geom_point(alpha = .5) + 
  geom_smooth() + 
    labs(x = "Revenue",
       y = "Number of Comments") +
  scale_x_log10() 
```

---

### Comparing nonprofits that did and did not comment on Dodd-Frank rules

```{r nonprofit-density-old, fig.height= 1.7, fig.width=4, out.width="50%"}
load(here("data", "nonprofit_resources.Rdata"))

nonprofit_resources %>% 
  filter(assets > 10) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = assets, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "", x = "Assets")

nonprofit_resources %>% 
  #filter(assets > 100) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = revenue, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "", y = "", x = "Revenue")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

# CHARTS 
# - who comments, who does not 
# - 


# TODO 
# - biggest nonprofits by assets and revenue (e.eg. chamber )
# - compartaitve ratios at key levels
# - percent nonprofits 
# - side by side histograms
# missing fed in matching, but could compare FDIC regulator to agency commented on
# - scholzman and verba, gilens and bartels 
```

Same plots with new data (finreg_org_level_covariates_df_20210908.csv)

```{r nonprofit-density, fig.height=1.5, fig.width=4, out.width="50%", cache=FALSE}
load(here("data", "nonprofit_resources.Rdata"))

nonprofit_resources %<>% 
  mutate(Commented = ifelse(
    org_name %in% d$best_match_name,
    "Commented", 
    "Did not comment")) 

nonprofit_resources %>% 
  group_by(Commented) %>% 
  summarise(mean_assets = mean(assets, na.rm = T))

nonprofit_resources %>% 
  filter(assets > 10) %>% 
  ggplot() + 
  aes(x = assets/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Nonprofits",
       fill = "", y = "", x = "Assets ($1,000s)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

nonprofit_resources %>% 
  filter(assets > 10) %>% 
  ggplot() + 
  aes(x = assets/1000, fill = Commented) + 
  geom_histogram(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Nonprofits",fill = "", x = "Assets ($1,000s)") + 
  facet_grid(Commented ~ ., scales = "free_y")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())


nonprofit_resources %>% 
  filter(revenue > 10) %>% 
  mutate(Commented = ifelse(commented, "Commented", "Did not comment")) %>%
  ggplot() + 
  aes(x = revenue/1000, fill = Commented) + 
  geom_histogram(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Nonprofits",fill = "", x = "Revenue ($1,000s)")+ 
  facet_grid(Commented ~ ., scales = "free_y")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())
```


---

## By Lobbying Spending

(OpenSecrets data)

```{r opensecrets-density, fig.height=1.5, fig.width=4, out.width="50%"}
# load the full population of campaign donors 
opensecrets <- read_csv(here("data" , "merged_resources", 
"opensecrets_resources_JwVersion.csv") )

# indicator for whether they commented
opensecrets %<>% 
  mutate(org_name = orgName %>% str_to_lower(),
         Commented = ifelse(
    org_name %in% d$best_match_name,
    "Commented", 
    "Did not comment"))

opensecrets %>% 
  group_by(Commented) %>% 
  summarise(mean_MeanContribAmount = mean(MeanContribAmount, na.rm = T),
            mean_MeanContribAmountPerYearContributed = mean(MeanContribAmountPerYearContributed, na.rm = T),
            MeanTotalContribAmount = mean(TotalContribAmount, na.rm = T))

opensecrets %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = MeanContribAmount/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "",  
       y = "",
       title = "Campaign Spending",
       x = "Mean Contribution
       ($1,000s, 2010-2017)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

opensecrets %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = MeanContribAmountPerYearContributed/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(title = "Campaign Spending",
       fill = "", 
       y = "",
       x = "Mean Contribution\n($1,000s, 2010-2017)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())

opensecrets %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = TotalContribAmount/1000, fill = Commented) + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  labs(fill = "", 
       title = "Campaign Spending",
       y = "",
       x = "Total Contributions\n($1,000s, 2010-2017)")+
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())
```


---

## By Market Cap

(Computstat data)

```{r compustat-density, fig.height=1.5, fig.width=4, out.width="50%"}
compustat <- read_csv(here("data/merged_resources/compustat_resources.csv"))


compustat %<>% 
  mutate(org_name = conm %>% str_to_lower(),
         Commented = ifelse(
    org_name %in% d$best_match_name,
    "Commented", 
    "Did not comment")) %>% 
  mutate(marketcap2 = ifelse(str_detect(marketcap, "k"),
                             marketcap %>% str_remove("k") %>% as.numeric() * 1000,
                             marketcap),
         marketcap2 = ifelse(str_detect(marketcap, "M"),
                             marketcap %>% str_remove("M") %>% as.numeric() * 1000000,
                             marketcap2),
         marketcap2 = ifelse(str_detect(marketcap, "B"),
                             marketcap %>% str_remove("B") %>% as.numeric() * 1000000000,
                             marketcap2),
         marketcap2 = ifelse(str_detect(marketcap, "T"),
                             marketcap %>% str_remove("T") %>% as.numeric() * 1000000000000,
                             marketcap2),
         marketcap2 = marketcap2 %>% str_remove(",") %>%  as.numeric(marketcap2))

#compustat %>% filter(is.na(marketcap2), !is.na(marketcap)) %>% pull(marketcap)

compustat %>% 
  group_by(Commented) %>%
  #mutate(mean = mean(marketcap2/1000, na.rm = T)) %>% 
  #filter(assets > 10) %>% 
  ggplot() + 
  aes(x = marketcap2/1000, fill = Commented) + 
  labs(fill = "", 
       y = "",
       title = "Banks and Bank-like Entities",
       x = "Market Capitalization\n($1,000s)") + 
  geom_density(alpha = .5, color = NA) + 
  scale_x_log10() + 
  #geom_vline(aes(xintercept = mean)) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_blank())
```



# Combine asset data with commenting behavior

```{r}
nrow(d)

d  %<>% 
  mutate(marketcap = coalesce(`CIK-bestMatch:marketcap`, `compustat_resources-bestMatch:marketcap`),
         marketcap2 = ifelse(str_detect(marketcap, "k"),
                             marketcap %>% str_remove("k") %>% as.numeric() * 1000,
                             marketcap),
         marketcap2 = ifelse(str_detect(marketcap, "M"),
                             marketcap %>% str_remove("M") %>% as.numeric() * 1000000,
                             marketcap2),
         marketcap2 = ifelse(str_detect(marketcap, "B"),
                             marketcap %>% str_remove("B") %>% as.numeric() * 1000000000,
                             marketcap2),
         marketcap2 = ifelse(str_detect(marketcap, "T"),
                             marketcap %>% str_remove("T") %>% as.numeric() * 1000000000000,
                             marketcap2),
         marketcap2 = marketcap2 %>% str_remove(",") %>%  as.numeric(marketcap2))

# d %<>% left_join(opensecrets) %>% 
#   left_join(nonprofit_resources) %>% 
#   left_join(compustat %>% select(-index)) 

d %<>% distinct()

d %<>% mutate(MeanContribAmount = `opensecrets_resources_jwVersion-bestMatch:MeanContribAmountPerYearContributed`,
              assets = coalesce( `FDIC_Institutions-bestMatch:ASSET`, `nonprofits_resources-bestMatch:assets`) )


#FIXME This is dumb
d %<>% mutate(org_resources = 
                coalesce(org_resources,
                         MeanContribAmount,
                         assets,
                         marketcap2))

#d$org_resources

d %>% distinct(comment_url) %>% nrow()
```

We only have market cap for  `r d %>% filter(!is.na(marketcap)) %>% count(best_match_name)  %>% nrow()` firms that submitted `r nrow(d %>% filter(!is.na(marketcap)))` comments.
```{r}
# fishy
d %>% 
  ggplot() +
  aes(x = marketcap2) + 
  geom_histogram()

d %>% arrange(-marketcap2) %>% count(marketcap, marketcap2, best_match_name, sort = T) %>% filter(!is.na(marketcap))

# unique(d$marketcap)
```



---
## Duplicates

(more than one match per comment URL)

Orgs in multiple states are one problem:

```{r duplicates}
# duplicates
d %>% 
  add_count(comment_url, name = "n_per_url") %>% 
  ungroup() %>% 
  filter(n_per_url > 1) %>% distinct() %>% 
  arrange(comment_url) %>%
  kablebox()
```

<!--
But we still have almost as many duplicates when we drop `state`, partially because assets vary for the same "org" registered in different states: 

```{r duplicates2, eval = FALSE }
# duplicates after removing state
d %>% 
  select(-state) %>%
  add_count(comment_url, name = "n_per_url") %>% 
  ungroup() %>% 
  filter(n_per_url > 1) %>% distinct() %>% 
  arrange(comment_url) %>%
  kablebox()
```

--> 

# Asset data coverage by docket

```{r}
is_not_na <- . %>% is.na() %>% isFALSE()


dtable <- d_rin_lookup %>%
  ungroup() %>% 
  full_join(d %>% select(docket_id, comment_url, ends_with("_name"))) %>%
  ungroup() %>% 
  # add_count(docket_id) %>% 
  group_by(comment_url) %>% 
  mutate(across(ends_with("name"), is_not_na)) %>% 
  mutate(asset_data_name = sum( CIK_name, CreditUnions_name, FDIC_Institutions_name,	FFIECInstitutions_name,	SEC_Institutions_name,	compustat_resources_name,	nonprofits_resources_name) > 0#,
         #unique_orgs_name = org_name %>% unique() %>% length() 
         ) %>% 
  select(docket_id, comment_count, comments_matched, lowest.sorted.rin.of.related.rins,related.rins,
         comment_url, ends_with("_name")) %>% 
  distinct() %>% 
  group_by(docket_id, comment_count, comments_matched, lowest.sorted.rin.of.related.rins,related.rins) %>% 
  # tally up matches
  summarise(across(ends_with("name"), sum)) %>% 
  # select vars for table 
  select(docket_id,  lowest.sorted.rin.of.related.rins,related.rins, comment_count, comments_matched, asset_data_name,
         #unique_orgs_name,
        nonprofits_resources_name,
         CIK_name, CreditUnions_name, FDIC_Institutions_name,	FFIECInstitutions_name,	SEC_Institutions_name,	compustat_resources_name) %>% 
  ungroup() %>%
  distinct() %>% 
  arrange(-asset_data_name) 



sum2 <- . %>% sum(na.rm = TRUE)

dtable %>% summarise(across(where(is.numeric), sum2))
  
dtable %>% write_csv(here::here("data", "matches_per_rin.csv"))
```


## Save asset data
```{r save}
commenter_assets <- d

save(commenter_assets, file = here::here("data", "commenter_assets.Rdata"))
```


## Comments per rule

### Percent of comments by each organization type on any one rule
```{r}
# comments 
# of the matched sample 
# 90% of credit union comments, half of nonprofits, and 75% campaig
d %>% 
  mutate(docket_id = str_split(docket_id, ";")) %>% 
  unnest(docket_id) %>% 
    add_count(org_type, name = "dataset_total") %>% 
  count(org_type, docket_id, dataset_total, sort = T) %>%
  mutate(percent = percent( n / dataset_total , accuracy = 1)) %>% 
  kablebox()
```


### Percent of each organization type on any one rule

```{r}
# orgs (15% of nonprifits on payday loan rule, 30% of opensecrets)
d %>% 
 mutate(docket_id = str_split(docket_id, ";")) %>% 
  unnest(docket_id) %>% 
  # org rather than comment level counts now
  distinct(org_name, docket_id, org_type) %>% 
  # total orgs of each type  in dataset 
  add_count(org_type, name = "dataset_total") %>% 
  # number of each org type by docket 
  count(org_type, docket_id, `dataset_total`, sort = T) %>%
  mutate(percent = percent( n / dataset_total , accuracy = 1)) %>% 
  kablebox()
```

### Most Frequently Commeting Orgs
```{r}
# 
d %>% 
  # filter(comment_agency == "CFPB") %>% 
  # mutate(docket_id = comment_url %>% str_extract("CFPB-[0-9]*-[0-9]*")) %>% 
  count(org_name, best_match_name, org_type, docket_id, sort = T) %>% kablebox()
```


# Sophistication 


Fixing comment urls to merge data
```{r assets-bluebook}
load(here("data", "bluebook.Rdata"))
load(here("data", "tech_iterations.RData"))

technical_terms %<>% mutate(comment_url = doc_id)

# names(bluebook)
# bluebook %>% count(comment_url, sort = T)

d %>% #filter(Agency != "CFPB") %>%
  group_by(Agency) %>% 
  slice_head(n = 1) %>%
  select(comment_url) %>% kablebox()

  
fix_url <- . %>%
  mutate(
  comment_url = gsub(pattern = "https:/www.sec.gov", 
                     replacement = "https://www.sec.gov", 
                     x = comment_url),
  comment_url = gsub(pattern = "S7-", 
                   replacement = "s7-", 
                   x = comment_url),
  # Federal Reserve Fix
  comment_url = gsub(pattern = "https:\\/\\/www\\.federalreserve\\.gov\\/SECRS\\/\\d{4}\\/\\w+/\\d{1,}/", 
                   replacement = "",
                   x = comment_url)
  )


d %<>% fix_url()

bluebook %<>% fix_url()

technical_terms %<>% fix_url()

bluebook %>% kablebox()

technical_terms %>% kablebox()
```

Technical terms N = 
```{r techical}
# matches 
d %>% filter(comment_url %in% technical_terms$comment_url) %>% 
  count(Agency) %>% kablebox()
```

Bluebook terms N = 
```{r bluebook}
# 6 k in both sets 
d %>% filter(comment_url %in% bluebook$comment_url) %>%
  count(Agency) %>% kablebox()

bluebook %>% filter(comment_url %in% d$comment_url) %>% nrow()
```

## Assets x bluebook terms
```{r assets-blue, fig.height=2, fig.width=3, out.width="50%"}
# bluebook %>% count(is.na(Total_Legal_Citations))

# 400 from FDIC data, 400 from other data
b <- bluebook %>% 
  left_join(d %>% 
  select(comment_url, org_name, comment_agency, org_resources, MeanContribAmount,
                         assets,
                         marketcap2) %>% 
    distinct() 
) %>% 
  drop_na(Total_Legal_Citations)
# d %>% count(comment_url, sort = T) 

b %<>% distinct()

b %>% select(org_name, Total_Legal_Citations, org_resources, comment_url)  %>%  
  filter(!is.na(org_name)) %>%
  kablebox()

# FDIC ASSTS 
b %>% 
  ggplot() +
  aes(x = org_resources, # %>% log(), 
      y = Total_Legal_Citations) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

# nonprofits
b %>% 
  ggplot() +
  aes(x = assets, 
      y = Total_Legal_Citations) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

# contribution
b %>% 
  ggplot() +
  aes(x = MeanContribAmount, 
      y = Total_Legal_Citations) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

b %>% 
  ggplot() +
  aes(x = marketcap2, 
      y = Total_Legal_Citations) + 
  geom_jitter() + 
  geom_smooth(method = "lm"
  ) 

b %>% count(comment_agency, is.na( assets))

b %>% filter(Total_Legal_Citations>10,
             !is.na(org_name)) %>% select(org_name,Total_Legal_Citations, comment_url, everything()) %>% arrange(-Total_Legal_Citations) %>% kablebox()
```

## Assets x technical terms

```{r assets-tech, fig.height=2, fig.width=3, out.width="50%"}
t <- technical_terms %>% 
  left_join(d %>% 
              select(comment_url, org_name,
                     comment_agency, org_resources, MeanContribAmount,
                         assets,
                         marketcap2,
                     `FDIC_Institutions-bestMatch:ASSET`) %>% 
    distinct() 
) %>% 
  drop_na(dictionary_terms)


# FDIC ASSTS 
t %>% 
  filter(!is.na(`FDIC_Institutions-bestMatch:ASSET`)) %>% 
  ggplot() +
  aes(x = `FDIC_Institutions-bestMatch:ASSET`, # %>% log(), 
      y = dictionary_terms ) + 
  geom_point(alpha = .5) + 
  geom_smooth(method = "lm"
  ) + 
  scale_y_log10() +
  scale_x_log10() + 
  labs(x = "Assets",
       y = "Legal and Technial Terms", 
       title = "FDIC-Insured Banks")

t %>% 
  filter(!is.na(`FDIC_Institutions-bestMatch:ASSET`), dictionary_terms == 0) %>%
  select(org_name, everything(), -doc_id) %>% 
  kablebox()

# nonprofits
t %>% 
  filter(!is.na(assets)) %>% 
  ggplot() +
  aes(x = assets, 
      y = dictionary_terms) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

# contribution
t %>% 
  filter(!is.na(MeanContribAmount)) %>%
  ggplot() +
  aes(x = MeanContribAmount, 
      y = dictionary_terms) + 
  geom_point() + 
  geom_smooth(method = "lm"
  )

t %>% 
  filter(!is.na(marketcap2)) %>% 
  ggplot() +
  aes(x = marketcap2, 
      y = dictionary_terms) + 
  geom_jitter() + 
  geom_smooth(method = "lm"
  ) 

## CRROSSWALK ASSET DATA  WITH TECHNICAL TERMS
# names(t) %<>% str_remove("_name") %>% str_remove("_resources")

dtable %>%
  #filter(asset_data > 0) %>% 
  #distinct(docket_id)
  kablebox()

# sum(t$asset_data[1:6])

# t %>% tally(comments_matched)
# d %>% distinct(comment_url)

# t %>% tally(asset_data)

# dtable %>% 
#   ggplot() + 
#   aes(x = docket_id) + 
#   geom_col(aes(y = asset_data)) 

```



# Regressions

## Probability of Commenting

These models have the prefix `mp_`. The dependent variable is probability that the organization comments.

### FDIC-insured Banks

SM = Comptroller of the Currency (OCC). National Charter and a Fed member.

NM = Commercial Bank. Bank is supervised by the Federal Deposit Insurance Corporation (FDIC). State charter and Fed non-member

SB =Savings Bank. Bank is supervised by the Federal Deposit Insurance Corporation (FDIC). State Charter.

OI = Bank is an insured United States branch of a foreign chartered institution (IBA)

```{r mpFDIC}
FDIC_resources %<>% mutate(Class = str_c(" ", BKCLASS),
                           Assets = ASSET)

cm = c("ASSET" = "Assets")

mpFDIC <- glm(commented ~ Assets + Class,
           data = FDIC_resources, 
             family=binomial(link="logit"))

mpFDIC_BKCLASS <- glm(commented ~ Assets*Class,
           data = FDIC_resources, 
             family=binomial(link="logit"))


rows <- tibble(
  term = c("Dependent Variable"),
  `1` = "Comment",
  `2` = "Comment"
)

attr(rows, 'position') <- c(0)

models <- list(mpFDIC, mpFDIC_BKCLASS)

equatiomatic::extract_eq(models[[1]]) 

modelsummary(models)

modelplot(models[[1]] ) +
  labs(title  = "SM = State Bank
SB = Savings Bank
SA = Savings Association
NM = Commercial Bank",
x = "Effect on Log Odds of Commenting")
```


### Nonprofits

```{r mpNonprofit}
nonprofit_resources %<>% mutate(Assets = assets)

mpNonprofit <- glm(commented ~ assets,
           data = nonprofit_resources , 
             family=binomial(link="logit"))

mpNonprofit_revenue <- glm(commented ~ assets + revenue,
           data = nonprofit_resources, 
             family=binomial(link="logit"))

models <- list(mpNonprofit, mpNonprofit_revenue)

equatiomatic::extract_eq(models[[1]]) 

modelsummary(models)

modelplot(models)
```

### Campaign Donors

```{r mpOpensecrets}
opensecrets %<>% mutate(commented = Commented == "Commented")

mpOpensecrets <- glm(commented ~ MeanContribAmount,
           data = opensecrets, 
             family=binomial(link="logit"))

mpOpensecrets_total <- glm(commented ~ TotalContribAmount,
           data = opensecrets, 
             family=binomial(link="logit"))

models <- list(mpOpensecrets, mpOpensecrets_total)

equatiomatic::extract_eq(models[[1]]) 

modelsummary(models)

# modelplot(models)
```

### By Market Cap
```{r mpCompustat}
compustat %<>% mutate(commented = Commented == "Commented")

cm = c("marketcap2" = "Market Capitalization")

mpCompustat <- glm(commented ~ marketcap2,
           data = compustat, 
             family=binomial(link="logit"))

top_naics <- compustat %>% count(naics, sort = T) %>% 
  drop_na(naics) %>% 
  top_n(10, wt = n) %>% 
  pull(naics)

mpCompustat_naics <- glm(commented ~ NAICS_,
           data = compustat %>% mutate(NAICS_ = ifelse(naics %in% top_naics, naics, "other")), 
             family=binomial(link="logit"))

models <- list(mpCompustat, mpCompustat_naics)

equatiomatic::extract_eq(models[[1]]) 

modelsummary(models)

# modelplot(models)

# modelplot(mpCompustat)
 
# modelplot(mpCompustat_naics)
```

---

### Variation Within and Across Types of Commenters 

```{r mp}
# Pr(Commenting) ~ assets + org_type + assets*org_type 

d %<>% mutate(org_is_ = org_type)

cm = c("marketcap2" = "Market Capitalization")

mp <- glm(commented ~ org_resources,
           data = d  %>% 
            # dropping FDIC due to extreme variance
            #filter(org_is_ != "FDIC") %>%
            mutate(org_resources = org_resources/10000), 
             family=binomial(link="logit"))

mp_type <- glm(commented ~ org_resources*org_is_,
           data = d  %>% 
            #filter(org_is_ != "FDIC") %>%
             mutate(org_resources = org_resources/10000) , 
             family=binomial(link="logit"))

models <- list(mp, mp_type)

equatiomatic::extract_eq(models[[1]]) 

modelsummary(models)

#  modelplot(models)
```

---


## Sophistication

> NOTE: I'm using OLS for now, but will switch to Poisson or Negative Binomial. 

### Bluebook
These models have the prefix `mb_`. The dependent variable is a count of technical terms per comment.


```{r mb}
rows <- tibble(
  term = c("Dependent Variable"),
  `1` = "Sophistication",
  `2` = "Sophistication",
  `3` = "Sophistication"
)

# mb <- lm(Total_Legal_Citations ~ org_resources,
#            data = b)
# 
# mb_agency_only <- glm(Total_Legal_Citations ~ comment_agency,
#            data = b)
# 
# mb_agency <- glm(Total_Legal_Citations ~ org_resources*comment_agency,
#            data = b)
# 
# models <- list(mb, mb_agency, mb_agency_only)
# 
# equatiomatic::extract_eq(models[[1]]) 
# 
# modelsummary(models)
# 
# # modelplot(models)


# broken out by org type 

mb_assets <- lm(Total_Legal_Citations ~ assets,
           data = b  %>% mutate(assets = assets/10000000))

mb_opensecrets <- glm(Total_Legal_Citations ~ MeanContribAmount,
           data = b %>% mutate(MeanContribAmount = MeanContribAmount/10000))

mb_compustat <- glm(Total_Legal_Citations ~ marketcap2,
           data = b  %>% mutate(marketcap2 = marketcap2/10000000))

models <- list(mb_assets, mb_compustat)

modelsummary(models)

modelplot(mb_opensecrets) + 
  labs(x = "Additional Bluebook Terms per Ten Thousand Dollars")

modelplot(models) + 
  labs(x = "Additional Bluebook Terms per Ten Million Dollars")
```

### Technical Terms

These models have the prefix `mt_`. The dependent variable is a count of technical terms per comment.

```{r mt}
# t %<>% select(-org)
# broken out by org type 

mt_assets <- lm(dictionary_terms ~ assets,
           data = t  %>% mutate(assets = assets/10000000))

mt_opensecrets <- glm(dictionary_terms ~ MeanContribAmount,
           data = t %>% mutate(MeanContribAmount = MeanContribAmount/10000))

mt_compustat <- glm(dictionary_terms ~ marketcap2,
           data = t  %>% mutate(marketcap2 = marketcap2/10000000))

models <- list(mt_assets, mt_compustat)

modelsummary(models)

modelplot( mt_opensecrets) + 
  labs(x = "Additional Technical and Legal Terms per Ten Thousand Dollars")

modelplot(models) + 
  labs(x = "Additional Technical and Legal Terms per Ten Million Dollars")


```

---

## Efficacy

These models have the prefix `me_`. The dependent variable is efficacy score.

---

### Efficacy Mediated by Technical Terms

These models have the prefix `memt-`. The dependent variable is efficacy score. The mediator is technical terms.


---

# Steve's old code for reference
```{r, eval=FALSE}
# Steve's code
# https://www.dropbox.com/s/dt7n0a771jngpte/Efficacy_Plots.R?dl=0
######################################

'%!in%' <- function(x,y)!('%in%'(x,y))

library(scales) # to access break formatting functions
library(DBI)
library(RSQLite)
library(quanteda)
library(tidyverse)
library(magrittr)
library(tidylog)
library(stargazer)
library(corrplot)



all_repeated_language <- read.csv(here("data/repeated_language.csv"), stringsAsFactors = F, 
                                  strip.white = T, sep = ",")

all_repeated_language %<>% as_tibble()

commenter_covariates <- read.csv(file = here("data/finreg_commenter_covariates_df_20210824.csv"), 
                                 strip.white = T, stringsAsFactors = F)

# federal reserve weird in sqlite and that translates to this frame
commenter_covariates %<>%
  mutate(
  comment_url = gsub(pattern = "https:/www.sec.gov", 
                     replacement = "https://www.sec.gov", 
                     x = comment_url),
  comment_url = gsub(pattern = "S7-", 
                   replacement = "s7-", 
                   x = comment_url),
  # Federal Reserve Fix
  comment_url = gsub(pattern = "https:\\/\\/www\\.federalreserve\\.gov\\/SECRS\\/\\d{4}\\/\\w+/\\d{1,}/", 
                   replacement = "",
                   x = comment_url)
  )
  
load(here("data", "tech_iterations.RData"))
load(here("data", "bluebook.RData"))



# Efficacy Measure Summary Stats and Plots ####
technical_terms %<>%
  mutate(  comment_url = doc_id) %>%
  left_join(Tables_and_Figures, by = "comment_url") %>%
  left_join(bluebook, by = "comment_url") %>%
  mutate(
  # SEC fix
  comment_url = gsub(pattern = "https:/www.sec.gov", 
                     replacement = "https://www.sec.gov", 
                     x = comment_url),
  comment_url = gsub(pattern = "S7-", 
                   replacement = "s7-", 
                   x = comment_url),
  # Federal Reserve Fix
  comment_url = gsub(pattern = "https:\\/\\/www\\.federalreserve\\.gov\\/SECRS\\/\\d{4}\\/\\w+/\\d{1,}/", 
                   replacement = "",
                   x = comment_url)
  )
  
technical_terms %>%
  filter(grepl(pattern = "sec.gov",x = comment_url))

missing_from_all_repeated <- all_repeated_language %>%
  anti_join(technical_terms, by = "comment_url")

missing_from_technical_terms <- technical_terms %>%
  anti_join(all_repeated_language, by = "comment_url")

missing_from_technical_terms
  
all_repeated_language %>%
  left_join(technical_terms, by = "comment_url")




## Rename Efficacy Measure variables for plots ####
all_repeated_language %<>%
  
  # Plag sequences
  rename("Num. Seq. Over 5" = "difference_plagiarised_number_of_other_sequences_over_5") %>%
  rename("Num. Seq. Over 10" = "difference_plagiarised_number_of_other_sequences_over_10") %>%
  rename("Num. Seq. Over 25" = "difference_plagiarised_number_of_other_sequences_over_25") %>%
  rename("Num. Seq. Over 50" = "difference_plagiarised_number_of_other_sequences_over_50") %>%
  rename("Num. Seq. Over 100" = "difference_plagiarised_number_of_other_sequences_over_100") %>%
  
  # Plag length
  rename("Seq. Leng. Over 5" = "difference_plagiarised_cumulative_sequence_length_over_5") %>%
  rename("Seq. Leng. Over 10" = "difference_plagiarised_cumulative_sequence_length_over_10") %>%
  rename("Seq. Leng. Over 25" = "difference_plagiarised_cumulative_sequence_length_over_25") %>%
  rename("Seq. Leng. Over 50" = "difference_plagiarised_cumulative_sequence_length_over_50") %>%
  rename("Seq. Leng. Over 100" = "difference_plagiarised_cumulative_sequence_length_over_100") %>%
  
  # Fuzzy sequences
  rename("Fuzzy Num. Seq. Over 10" = "differece_fuzzy_number_of_other_sequences_over_10") %>%
  rename("Fuzzy Num. Seq. Over 25" = "differece_fuzzy_number_of_other_sequences_over_25") %>%
  rename("Fuzzy Num. Seq. Over 50" = "differece_fuzzy_number_of_other_sequences_over_50") %>%
  rename("Fuzzy Num. Seq. Over 100" = "differece_fuzzy_number_of_other_sequences_over_100") %>%
  
  # Fuzzy length
  rename("Fuzzy Seq. Leng. Over 10" = "differece_fuzzy_cumulative_sequence_length_over_10") %>%
  rename("Fuzzy Seq. Leng. Over 25" = "differece_fuzzy_cumulative_sequence_length_over_25") %>%
  rename("Fuzzy Seq. Leng. Over 50" = "differece_fuzzy_cumulative_sequence_length_over_50") %>%
  rename("Fuzzy Seq. Leng. Over 100" = "differece_fuzzy_cumulative_sequence_length_over_100") %>%
  
  # Other terms
  rename("Unique Tokens" = "Types") %>%
  rename("Total Tokens" = "Tokens") %>%
  rename("Total Sentences" = "Sentences") %>%
  rename("Words" = "words") 

order_of_terms_for_plots <- c(
  "Unique Tokens", "Total Tokens", "Total Sentences", "Words",
  "Num. Seq. Over 5", "Num. Seq. Over 10",
  "Num. Seq. Over 25", "Num. Seq. Over 50",
  "Num. Seq. Over 100",
  "Seq. Leng. Over 5", "Seq. Leng. Over 10",
  "Seq. Leng. Over 25", "Seq. Leng. Over 50",
  "Seq. Leng. Over 100",
  "Fuzzy Num. Seq. Over 10","Fuzzy Num. Seq. Over 25",
  "Fuzzy Num. Seq. Over 50", "Fuzzy Num. Seq. Over 100",
  "Fuzzy Seq. Leng. Over 10", "Fuzzy Seq. Leng. Over 25",
  "Fuzzy Seq. Leng. Over 50", "Fuzzy Seq. Leng. Over 100")

data_for_plots <- all_repeated_language %>%
  gather(terms, val, -comment_url) %>%
  mutate(val = as.numeric(val)) %>%
  group_by(terms) %>%
  filter(terms %in% order_of_terms_for_plots) %>%
  mutate(terms = factor(terms, levels=c(
    "Unique Tokens",
    "Total Tokens",
    "Total Sentences",
    "Words",
    "Num. Seq. Over 5",
    "Num. Seq. Over 10",
    "Num. Seq. Over 25",
    "Num. Seq. Over 50",
    "Num. Seq. Over 100",
    "Seq. Leng. Over 5",
    "Seq. Leng. Over 10",
    "Seq. Leng. Over 25",
    "Seq. Leng. Over 50",
    "Seq. Leng. Over 100",
    "Fuzzy Num. Seq. Over 10",
    "Fuzzy Num. Seq. Over 25",
    "Fuzzy Num. Seq. Over 50",
    "Fuzzy Num. Seq. Over 100",
    "Fuzzy Seq. Leng. Over 10",
    "Fuzzy Seq. Leng. Over 25",
    "Fuzzy Seq. Leng. Over 50", 
    "Fuzzy Seq. Leng. Over 100")))

#### Summary Stats ####
summary_stats <- data_for_plots %>%
  summarise(
    n = n(),
    min = fivenum(val)[1],
    #Q1 = fivenum(val)[2],
    median = fivenum(val)[3],
    #Q3 = fivenum(val)[4],
    Q90 = quantile(val, 0.9, na.rm = T),
    Q95 = quantile(val, 0.95, na.rm = T),
    Q99 = quantile(val, 0.99, na.rm = T),
    max = fivenum(val)[5]
  ) 

summary_stats %>%
  pander::pandoc.table()

#### Export Summary Stats to Latex ####
print(xtable::xtable(summary_stats, digits = 0), include.rownames = F)

# Violin Plots ####

#### Length ####
comment_length <- data_for_plots %>% 
  filter(terms %in% c("Total Sentences","Words")) %>%
  mutate(val = val + 1) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  facet_wrap(~terms, scales = "free") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_minimal() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Comment Length",
       caption = "Sentences measured from capital letter to period; Words measured as characters between spaces") +  
  theme(legend.position="none",
        plot.title = element_text(size = 12, face = "bold",hjust = 0.5)) +  
  xlab("") +
  ylab("Count")

#### Exact Matches: Number ####
plag_seq <- data_for_plots %>%
  filter(terms %in% c(                        "Num. Seq. Over 5",
                                              "Num. Seq. Over 10",
                                              "Num. Seq. Over 25",
                                              "Num. Seq. Over 50",
                                              "Num. Seq. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Exact Matches: Number of Sequences over Length n in Comment and Final Rule",
       caption = "These are counts; a 1 where n = 5 represents 5 words in the comment and final rule") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Count of Exact Sequences")

#### Exact Matches: Length ####
plag_len <- data_for_plots %>%  
  filter(terms %in% c("Seq. Leng. Over 5",
                      "Seq. Leng. Over 10",
                      "Seq. Leng. Over 25",
                      "Seq. Leng. Over 50",
                      "Seq. Leng. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Exact Matches: Total Length of Matched Sequences",
       caption = "These are the cumulative lengths of the sequences above per document") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Total Amount of Repeated Text")

#### Fuzzy Matches: Number ####
fuzzy_seq <- data_for_plots %>%  
  filter(terms %in% c("Fuzzy Num. Seq. Over 10",
                      "Fuzzy Num. Seq. Over 25",
                      "Fuzzy Num. Seq. Over 50",
                      "Fuzzy Num. Seq. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Fuzzy Matches: Number of Sequences over Length n in Comment and Final Rule",
       caption = "These are counts; a 1 where n = 5 represents 5 words in the comment and final rule") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Count of Fuzzy Mathed Sequences")

#### Fuzzy Matches: Length ####
fuzzy_len <- data_for_plots %>%  
  filter(terms %in% c("Fuzzy Seq. Leng. Over 10",
                      "Fuzzy Seq. Leng. Over 25",
                      "Fuzzy Seq. Leng. Over 50", 
                      "Fuzzy Seq. Leng. Over 100")) %>%
  group_by(terms) %>%
  ggplot( aes(x=terms, y=val)) + #, fill=terms)) +
  geom_violin(width=1) +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  geom_boxplot(width=0.2, color="grey", alpha=0.2) +
  labs(title = "Fuzzy Matches: Total Length of Matched Sequences",
       caption = "These are the cumulative lengths of the sequences above per document") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("") +
  ylab("Total Amount of Repeated Text")

## Save Distribution Plots ####
plag_seq
plag_len
fuzzy_seq
fuzzy_len

# ggsave(plot = plag_seq, "violin_plag_seq.pdf", width = 9, height = 6.5)
# ggsave(plot = plag_seq, "violin_plag_len.pdf", width = 9, height = 6.5)
# ggsave(plot = fuzzy_len, "violin_fuzzy_seq.pdf", width = 9, height = 6.5)
# ggsave(plot = fuzzy_seq, "violin_fuzzy_seq.pdf", width = 9, height = 6.5)

# Correlation Plots ####
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  return(p.mat)
}

assemble_corr_plots <- function(tbl){
  # tbl = data_for_correlation_plot 
  
  #names(tbl) <- base::tolower(substring(names(tbl), 1,5))
  
  corrs <- tbl %>%
    cor()
  
  p.mat <-  tbl %>%
    cor.mtest()
  
  col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
  
  c_plot <- function() {
    corrplot(corrs, method="color", col=col(200),  
             type="upper", order="hclust", 
             addCoef.col = "black", # Add coefficient of correlation
             tl.col="black", tl.srt=45, #Text label color and rotation
             # Combine with significance
             p.mat = p.mat, sig.level = 0.01, insig = "blank", 
             # hide correlation coefficient on the principal diagonal
             diag=FALSE)
  }
  list(c_plot = c_plot)
}

## Length ####
length_variables <- all_repeated_language %>%
  select(`Unique Tokens`,`Total Tokens`,
         `Total Sentences`, `Words`) 

pdf(file = "Length_Correlations.pdf")
assemble_corr_plots(length_variables)$c_plot()
dev.off()

plagiarism_number_variables <- all_repeated_language %>%
  select(`Num. Seq. Over 5`,`Num. Seq. Over 10`,`Num. Seq. Over 25`,
         `Num. Seq. Over 50`, `Num. Seq. Over 100`) 

pdf(file = "Plagiarism_Number_Correlations.pdf")
assemble_corr_plots(plagiarism_number_variables)$c_plot()
dev.off()

plagiarism_length_variables <- all_repeated_language %>%
  select(`Seq. Leng. Over 5`, `Seq. Leng. Over 10`,
         `Seq. Leng. Over 25`,`Seq. Leng. Over 50`,
         `Seq. Leng. Over 100`) 

#pdf(file = "Plagiarism_Length_Correlations.pdf")
#assemble_corr_plots(plagiarism_length_variables)$c_plot()
dev.off()

fuzzy_number_variables <- all_repeated_language %>%
  select(`Fuzzy Num. Seq. Over 10`,
         `Fuzzy Num. Seq. Over 25`,
         `Fuzzy Num. Seq. Over 50`,
         `Fuzzy Num. Seq. Over 100`) 

pdf(file = "Fuzzy_Number_Correlations.pdf")
#assemble_corr_plots(fuzzy_number_variables)$c_plot()
dev.off()

### Fuzzy Length of Sequences ###
fuzzy_length_variables <- all_repeated_language %>%
  select(`Fuzzy Seq. Leng. Over 10`,
         `Fuzzy Seq. Leng. Over 25`,
         `Fuzzy Seq. Leng. Over 50`,
         `Fuzzy Seq. Leng. Over 100`) 

pdf(file = "Fuzzy_Length_Correlations.pdf")
#assemble_corr_plots(fuzzy_length_variables)$c_plot()
dev.off()

all <- all_repeated_language %>%
  select(`Unique Tokens`,`Total Tokens`,
         `Total Sentences`, `Words`,
    `Num. Seq. Over 5`,`Num. Seq. Over 10`,`Num. Seq. Over 25`,
         `Num. Seq. Over 50`, `Num. Seq. Over 100`,
         `Seq. Leng. Over 5`, `Seq. Leng. Over 10`,
         `Seq. Leng. Over 25`,`Seq. Leng. Over 50`,
         `Seq. Leng. Over 100`,
         `Fuzzy Num. Seq. Over 10`,
         `Fuzzy Num. Seq. Over 25`,
         `Fuzzy Num. Seq. Over 50`,
         `Fuzzy Num. Seq. Over 100`,
         `Fuzzy Seq. Leng. Over 10`,
         `Fuzzy Seq. Leng. Over 25`,
         `Fuzzy Seq. Leng. Over 50`,
         `Fuzzy Seq. Leng. Over 100`) 

pdf(file = "All_Correlations.pdf",width=11, height=8.5, paper = "USr")
#assemble_corr_plots(all)$c_plot()
dev.off()

rm(all, fuzzy_length_variables,fuzzy_number_variables,
   plagiarism_length_variables,plagiarism_number_variables)

# Create two dataframes, one wide and one long ####

wide<- all_repeated_language %>%
  inner_join(commenter_covariates, by = "comment_url")

### Lots of missing comments in commenter_covariates
missing <- all_repeated_language %>%
  anti_join(commenter_covariates, by = "comment_url") 

# missing %>% View()

commenter_covariates$comment_url[201316]

### see one example here:
grep(pattern = "CFPB-2016-0020-0044", x = all_repeated_language$comment_url)
grep(pattern = "CFPB-2016-0020-0044", x = commenter_covariates$comment_url)

#### Asset Scatterplots ####
mdc_and_efficacy <- all_repeated_language %>%
  inner_join(match_data_clean, by = "comment_url") 

Words <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=Words)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Comment Length and Organizational Resources",
       caption = "") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Words")


Plag_Count_Over_10 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Num. Seq. Over 10`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Number of Repeated Sequences over 10 and Organizational Resources",
       caption = "The depentend variable is count") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")

Plag_Count_Over_25 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Num. Seq. Over 25`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Number of Repeated Sequences over 25 and Organizational Resources",
       caption = "The depentend variable is count") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")

Plag_Length_Over_10 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Seq. Leng. Over 10`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Cumulative Length and Organizational Resources",
       caption = "The depentend variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")

Plag_Length_Over_25 <- mdc_and_efficacy %>%
  ggplot( aes(x=org_resources, y=`Seq. Leng. Over 25`)) + #, fill=terms)) +
  geom_point() +
  geom_smooth() +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Cumulative Length and Organizational Resources",
       caption = "The depentend variable is based on sequences of over 25 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5)) +  
  xlab("Assets") +
  ylab("Total Amount of Repeated Text")


# master_location <- "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/attachments.sqlite"
# con = dbConnect(SQLite(), dbname=master_location)
# myQuery <- dbSendQuery(con, "SELECT * FROM attachments")
# All_comment_text <- dbFetch(myQuery, n = -1) %>% as_tibble()

# All_comment_text %<>%  arrange(comment_url)

check_no_words <- mdc_and_efficacy %>%
  select(comment_url, Words, org_resources) %>%
  filter(Words == 0) %>%
  select(comment_url) %>%
  pull()

# All_comment_text %>%
#   filter(comment_url %in% check_no_words)

# rm(All_comment_text)

Words
Plag_Count_Over_10
Plag_Count_Over_25
Plag_Length_Over_10
Plag_Length_Over_25
# ggsave(plot = Words, "Assets_Words.pdf", width = 9, height = 6.5)
# ggsave(plot = Plag_Count_Over_10, "Assets_Plag_Count_10.pdf", width = 9, height = 6.5)
# ggsave(plot = Plag_Count_Over_25, "Assets_Plag_Count_25.pdf",width = 9, height = 6.5)
# ggsave(plot = Plag_Length_Over_10, "Assets_Plag_Length_10.pdf", width = 9, height = 6.5)
# ggsave(plot = Plag_Length_Over_25, "Assets_Plag_Length_25.pdf", width = 9, height = 6.5)


repeated_text_scatter <- wide %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         `Seq. Leng. Over 10`, Words) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  gather(match_type, resources, -comment_url, -best_match_type, 
         -`Repeated Text`,
         -Words) %>%
  ggplot( aes(x=resources, y=`Repeated Text`)) + #fill = best_match_type, color = best_match_type
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~match_type, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Cumulative Repeated Text and Organizational Resources",
       caption = "The depentend variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Total Amount of Repeated Text")

repeated_text_by_agency <- wide %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         comment_agency,
         `Seq. Leng. Over 10`, Words) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  gather(match_type, resources, -comment_url, -best_match_type, 
         -`Repeated Text`,-comment_agency,
         -Words) %>%
  ggplot( aes(x=resources, y=`Repeated Text`)) + #fill = best_match_type, color = best_match_type
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Cumulative Repeated Text and Organizational Resources",
       caption = "The dependent variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Total Amount of Repeated Text (Words)")

# Plots for paper ####

# ggsave(plot = repeated_text_by_agency, "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Figures/Repeated_text_agency_assets_cumulative_over10.pdf", width = 13, height = 6.5)

# ggsave(plot = repeated_text_scatter, "Repeated_text_assets_cumulative_over10.pdf", width = 9, height = 6.5)

rpt_naive <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure", data = wide)
rpt_w_control <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)", data = wide)
rpt_w_control_w_fex <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
                          comment_agency + rin", data = wide)
rpt_w_control_w_full_fex <- lm(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
                          comment_agency + rin + best_match_type", data = wide)

# library(miceadds)

#rpt_w_control_se <- lm.cluster(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)", data = wide,cluster = 'rin')
#rpt_w_control_w_fex_se <- lm.cluster(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
#                          comment_agency + rin", data = wide,cluster = 'rin')
#rpt_w_control_w_full_fex_se <- lm.cluster(formula = "log(`Seq. Leng. Over 10`+1) ~ log_p1_financial_measure + log(Words+1)+
#                          comment_agency + rin + #best_match_type", data = wide,cluster = 'rin')
# 
# stargazer::stargazer(rpt_naive,rpt_w_control,rpt_w_control_w_fex, rpt_w_control_w_full_fex,
#                      title="Text Reuse and Organizational Assets", type="latex", 
#                      digits=3, 
#                      se = list(NULL, 
#                                sqrt(diag(vcov(rpt_w_control_se))), 
#                                sqrt(diag(vcov(rpt_w_control_w_fex_se))), 
#                                sqrt(diag(vcov(rpt_w_control_w_full_fex_se))) 
#                                ),
#                      dep.var.labels=c("Cumulative Repeated Text"),
#                      omit = c("comment_agency","rin","best_match_type"),
#                      covariate.labels=c("Log Org. Resources",
#                                         "Log Comment Length"),
#                      omit.stat=c("LL","ser","f","adj.rsq"), no.space=TRUE,
#                      add.lines = list(
#                        c("Clustering", "No", "Rule","Rule","Rule"),
#                        c("Fixed effects?", "No", "No","Agency, RIN","Agency, RIN, Source")),
#                      #out="/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Tables/efficacy_and_resources.txt"
)

# Complexity plots ####
tables_and_figures <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  ggplot( aes(x=Visualizations, y=`Repeated Text`)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  theme_bw() +
  labs(title = "Log Cumulative Repeated Text and Data",
       caption = "The dependent variable is based on sequences of over 10 words") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Tables and Figures") +
  ylab("Total Amount of Repeated Text (Words)")
  
# Dictionary Term plots ####
dict_terms <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  ggplot( aes(x=dictionary_terms, y=`Repeated Text`)) +
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Cumulative Repeated Text and Legal and Banking Terms",
       caption = "The dependent variable is based on sequences of over 10 words \n 
       Dictionary terms are the sum of banking and legal terms") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Legal and Banking Terms") +
  ylab("Total Amount of Repeated Text (Words)")

bluebook <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  rename("Repeated Text" = `Seq. Leng. Over 10`) %>%
  ggplot( aes(x=Total_Legal_Citations, y=`Repeated Text`)) +
  #Total_Legal_Citations = US_Code + Supreme_Court_Cases + 
  #Appeals_and_District_Court_Cases + Code_of_Federal_Regulations +
  #Federal_Register_Total
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Cumulative Repeated Text and Bluebook Citations",
       caption = "The dependent variable is based on sequences of over 10 words \n
       Bluebook terms are include US Code, Federal Court Cases, the CFR") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Bluebook Citations") +
  ylab("Total Amount of Repeated Text (Words)")

# ggsave(plot = dict_terms, "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Figures/Repeated_text_agency_dictionary_cumulative_over10.pdf", width = 13, height = 6.5)
# ggsave(plot = bluebook, "/Users/stevenrashin/Dropbox/FINREG-RULEMAKE/Drafts/Participatory Inequality and Rulemaking: Evidence from Financial Regulation/Figures/Repeated_text_agency_bluebook_cumulative_over10.pdf", width = 13, height = 6.5)

big_dict_resources <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         comment_agency,
         `Seq. Leng. Over 10`,dictionary_terms) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  gather(match_type, resources, -dictionary_terms,-comment_url, -best_match_type, 
         -comment_agency) %>%
  filter(resources>10000) %>%
  ggplot( aes(x=resources, y=dictionary_terms)) +
  #Total_Legal_Citations = US_Code + Supreme_Court_Cases + 
  #Appeals_and_District_Court_Cases + Code_of_Federal_Regulations +
  #Federal_Register_Total
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Dictionary Terms and Resources",
       caption = "") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Comment Sophistication (Dictionary Terms)")

all_dict_resources <- wide %>%
  left_join(technical_terms, by = c("comment_url")) %>%
  filter(Words > 0) %>%
  select(comment_url, best_match_type, matches("bestMatch.financial_measure"),
         comment_agency,
         `Seq. Leng. Over 10`,dictionary_terms) %>%
  distinct() %>%
  rename("CIK" = "CIK.bestMatch.financial_measure") %>%
  rename("FDIC" = "FDIC_Institutions.bestMatch.financial_measure") %>%
  rename("Compustat" = "compustat_resources.bestMatch.financial_measure") %>%
  rename("Nonprofits" = "nonprofits_resources.bestMatch.financial_measure") %>%
  rename("Open Secrets" = "opensecrets_resources_jwVersion.bestMatch.financial_measure") %>%
  gather(match_type, resources, -dictionary_terms,-comment_url, -best_match_type, 
         -comment_agency) %>%
  ggplot( aes(x=resources, y=dictionary_terms)) +
  #Total_Legal_Citations = US_Code + Supreme_Court_Cases + 
  #Appeals_and_District_Court_Cases + Code_of_Federal_Regulations +
  #Federal_Register_Total
  #dictionary_terms = banking + us_law - overlap
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~comment_agency, scales = "fixed") +
  scale_y_log10(labels = scales::comma_format(accuracy=1)) +
  scale_x_log10(labels = scales::comma_format(accuracy=10)) +
  theme_bw() +
  labs(title = "Log Dictionary Terms and Resources",
       caption = "") +  
  theme(legend.position="none",
        plot.title = element_text(hjust = 0.5),
        strip.background = element_rect(colour=NA, fill=NA),
        panel.border = element_rect(fill = NA, color = "black")) +  
  xlab("Resources") +
  ylab("Comment Sophistication (Dictionary Terms)")


names(technical_terms)

#'4. @Steve and @Chris, I’d recommend some scatters (colored by data source: “best_match_type”) 
#'and regressions of comment sophistication by log_p1_financial_measure 
#'(and in the organization-level dataset, by num_comments submitted and by 
#'num_venues_commenting_in)


#### DEAL WITH THIS LATER ####
long <- all_repeated_language %>%
  inner_join(commenter_covariates, by = "comment_url") 



```
